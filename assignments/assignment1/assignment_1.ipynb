{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a55655cba14c313eed90e50c1cdba913",
     "grade": false,
     "grade_id": "cell-d8b377aba23d9f3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "This is the second of three mandatory assignments which must be completed during the course. Note that you only need to pass 2 out of 3 assignments to be eligible for the exam.\n",
    "\n",
    "First some practical pieces of information:\n",
    "\n",
    "* When is the assignment due?: **23:59, Friday, August 5, 2022.**\n",
    "* Should i work with my group?: **Yes**. In particular, you should **only hand in 1 assignment per group and in a comment on Absalon write your group number and all group members**. \n",
    "\n",
    "The assignment consists of problems from the exercise sets that you have solved so far, problems from the exercises that have been modified a little to better suit the structure of the assignment and finally also new problems not seen in the exercises. \n",
    "\n",
    "**Note**: \n",
    "- It is important that you submit your edited version of this [notebook](https://fileinfo.com/extension/ipynb#:~:text=An%20IPYNB%20file%20is%20a,Python%20language%20and%20their%20data.) as a .ipynb file and nothing else. Do not copy your answers into another notebook that you have made. \n",
    "- Don't delete the empty non-editable (unless you specifically change the metadata) cells below each question. Those are hidden tests used by the `nbgrader` software to grade the assignment.\n",
    "- It is recommended to clone our [github repository](https://github.com/isdsucph/isds2022) and copy the entire `assignment1` folder to somewhere on your computer and complete the assignment in this folder.\n",
    "- It is good practice to always restart your notebook and run all cells before submitting or delivering your notebook to somebody else. This is to make sure that all cells run without raising any errors breaking the flow of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "459a25bfbfe70234fb99397dd7a844c4",
     "grade": false,
     "grade_id": "cell-e5576badd2b58d90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 2:\n",
    "\n",
    "This time we are going to **read the weather data from a csv file** located in this assignment directory instead of requesting the website.\n",
    "The file is called `weather_data_1870-1875.csv` and consists of weather data for the period 1870-1875. The csv file contains data which has been constructed by concatenating the _non-processed_ data from 1870-1875. In a later exercise we will need metadata about the stations so the weather data comes bundled inside a zip file called `data.zip` together with the metadata files. \n",
    "\n",
    "First, we want to create a folder to extract the data inside the zip file to. We'll use the [`Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path) object from the [`pathlib`](https://docs.python.org/3/library/pathlib.html) module to create our data folder. With the `Path` object we can construct new file paths by using the `/` operator. For instance, to create a new folder called `some_dir` located inside the directory containing this notebook we can write \n",
    "\n",
    "```python\n",
    "## Code snippet showing how to use the `/` operator\n",
    "# Create Path object of new folder located inside \n",
    "# the current working directory of this notebook\n",
    "fp = Path.cwd() / \"some_dir\"  \n",
    "# Use the Path object to actually create the subfolder\n",
    "Path.mkdir(fp, exist_ok=True)  \n",
    "```\n",
    "It is good practice to construct paths relative to the project directory. With `pathlib` this becomes easy, also across operating systems. If you are interested you can read more about the `pathlib` module [here](https://realpython.com/python-pathlib/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 2.X.1 (Not seen in module 2):**\n",
    "Use the code snippet above to create a subfolder located inside this directory named `data`. Store the path as a `Path` object inside the variable `fp_data`. We will use `fp_data` in the next exercise when extracting the zipfile's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15def5ae0510f32dca69b04ddc50b1ec",
     "grade": false,
     "grade_id": "2x1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fp_data = Path.cwd() / \"data\"\n",
    "Path.mkdir(fp_data, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bae59332888da39f84684680cc31fcde",
     "grade": true,
     "grade_id": "2x1-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ab3bf517ced19d3f422f2f65d15d918",
     "grade": false,
     "grade_id": "cell-4ae37c71df382dbd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.X.2 (Not seen in module 2):** Use the [`zipfile`](https://docs.python.org/3/library/zipfile.html) module to extract the content of `data.zip` to the subfolder created above. \n",
    "\n",
    "> _Hint:_ Use the [`extractall`](https://docs.python.org/3/library/zipfile.html#zipfile.ZipFile.extractall) method of the `ZipFile` object. See [here](https://thispointer.com/python-how-to-unzip-a-file-extract-single-multiple-or-all-files-from-a-zip-archive/) for a guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "028470c2eda880b8d38bfe16a40b71a2",
     "grade": false,
     "grade_id": "2x2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('data.zip') as zipObj:\n",
    "    zipObj.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c746efc3c12830df77e2f92b375f4d61",
     "grade": true,
     "grade_id": "2x2-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d5325888798d10692c986771969c91c",
     "grade": false,
     "grade_id": "cell-3949fc8a0311b795",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.3.4:** The code below runs through some of the steps we completed in exercise 2.3.4 in Module 2. As we are not going to request the website but load the data from a csv file, your task is to **rewrite parts of the function**. In particular, you need to do the following:`\n",
    ">1. Rename the function to `process_weather` instead of `load_weather`. \n",
    ">2. The function should now  take a `DataFrame` as input (the one we extracted from the zip file)\n",
    ">3. Consider whether `df_weather.iloc[:, :4]` is necessary for the weather data loaded from  the csv file. The documentation string should also be rewritten appropriately. \n",
    ">4. The function contains a sorting step. **Change it so that it first sorts by _station_, then by _datetime_. The sorting should be ascending for _station_ and descending for _datetime_.** \n",
    ">5. After having rewritten the function, load the weather data from `'weather_data_1870-1875.csv'` into a pandas dataframe, apply the `process_weather` function to this dataframe, and store the result in the variable `df_weather_period`.\n",
    "\n",
    "```python\n",
    "def load_weather(year):\n",
    "    \"\"\"Function to structure and clean weather data.\n",
    "    \n",
    "    Structuring includes removing unused columns, renaming the \n",
    "    columns and selecting only observations of maximum temperature. \n",
    "    Cleaning includes inserting missing decimal, sorting and\n",
    "    resetting the index.\n",
    "    \n",
    "    Args:\n",
    "        year (int): given year to load data from e.g. 1870\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame): processed weather data for given input year\n",
    "    \"\"\"\n",
    "    url = f\"ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/{year}.csv.gz\"\n",
    "\n",
    "    # loads the data\n",
    "    df_weather = pd.read_csv(url, header=None)\\\n",
    "                    .iloc[:,:4] \n",
    "\n",
    "    # structure and clean data using methods chaining\n",
    "    # note that the original columns now are strings when loading the csv file\n",
    "    # and not integers as when downloading the data\n",
    "    df_out = \\\n",
    "        df_weather\\\n",
    "            .rename(columns={'0': 'station', '1': 'datetime', '2': 'obs_type', '3': 'obs_value'})\\\n",
    "            .query(\"obs_type == 'TMAX'\")\\\n",
    "            .assign(obs_value=lambda df: df['obs_value']/10)\\\n",
    "            .sort_values(by=['station', 'datetime'])\\\n",
    "            .reset_index(drop=True)\\\n",
    "            .copy() \n",
    "\n",
    "    # area process\n",
    "    df_out['area'] = df_out['station'].str[0:2]\n",
    "\n",
    "    # datetime process\n",
    "    df_out['datetime_dt'] = pd.to_datetime(df_out['datetime'], format = '%Y%m%d')\n",
    "    df_out['month'] = df_out['datetime_dt'].dt.month\n",
    "    df_out['year'] = df_out['datetime_dt'].dt.year\n",
    "\n",
    "    return df_out\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0fcfb2b712a697a2c519e6f2d4102b6",
     "grade": false,
     "grade_id": "problem_234",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def process_weather(df_weather):\n",
    "    \"\"\"Function to structure and clean weather data.\n",
    "    \n",
    "    Structuring includes removing unused columns, renaming the \n",
    "    columns and selecting only observations of maximum temperature. \n",
    "    Cleaning includes inserting missing decimal, sorting and\n",
    "    resetting the index.\n",
    "    \n",
    "    Args:\n",
    "        year (int): given year to load data from e.g. 1870\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame): processed weather data for given input year\n",
    "    \"\"\"\n",
    "    # url = 'weather_data_1870-1875.csv'\n",
    "\n",
    "    # loads the data\n",
    "    #df_weather = pd.read_csv(fp_data / dataframe, header=None) \n",
    "\n",
    "    # structure and clean data using methods chaining\n",
    "    # note that the original columns now are strings when loading the csv file\n",
    "    # and not integers as when downloading the data\n",
    "    df_out = \\\n",
    "        df_weather\\\n",
    "            .rename(columns={'0': 'station', '1': 'datetime', '2': 'obs_type', '3': 'obs_value'})\\\n",
    "            .query(\"obs_type == 'TMAX'\")\\\n",
    "            .assign(obs_value=lambda df: df['obs_value']/10)\\\n",
    "            .sort_values(by=['datetime', 'station'])\\\n",
    "            .reset_index(drop=True)\\\n",
    "            .copy() \n",
    "\n",
    "    # area process\n",
    "    df_out['area'] = df_out['station'].str[0:2]\n",
    "\n",
    "    # datetime process\n",
    "    df_out['datetime_dt'] = pd.to_datetime(df_out['datetime'], format = '%Y%m%d')\n",
    "    df_out['month'] = df_out['datetime_dt'].dt.month\n",
    "    df_out['year'] = df_out['datetime_dt'].dt.year\n",
    "\n",
    "    return df_out\n",
    "\n",
    "df_weather_period = process_weather(pd.read_csv(fp_data / 'weather_data_1870-1875.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7189d84de812b64c7424088e3ca325b",
     "grade": true,
     "grade_id": "problem_234_tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78e78d64830c5518e7ef3173d94bf33c",
     "grade": false,
     "grade_id": "cell-7a8591d457df256a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.X.3 (Not seen in module 2):** Try to plot the observations value of `df_weather_period` by running `df_weather_period.obs_value.plot()`. Something seems off, right? Now try to inspect the problematic subset of the dataframe by running `df_weather_period[df_weather_period.obs_value < -50]`. What can these three observations be characterized as? Drop _all_ observations from the associated station from `df_weather_period`, reset the index and drop the column with the old index. Store the dataframe back into the variable `df_weather_period`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2de59076e97751d5e76fa532723f768",
     "grade": false,
     "grade_id": "problem_notseenexercises",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "df_weather_period.obs_value.plot()\n",
    "\n",
    "df_weather_period[df_weather_period.obs_value < -50]\n",
    "# These three observations can be characterized as outliers, why we do not want to include them in figure.\n",
    "# This suggests, that this weatherstation does not measure accurately why we want to drop this station from this dataset\n",
    "\n",
    "df_weather_period\\\n",
    "    .drop(df_weather_period[df_weather_period['station'] == 'USW00023068'].index, inplace=True)\\\n",
    "\n",
    "df_weather_period = df_weather_period.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5971a3b2c04c14fb5fb5f180e25ff481",
     "grade": true,
     "grade_id": "problem_notseenexercises_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1b79752e5634da4d89aa3ae634563e0",
     "grade": false,
     "grade_id": "cell-c2f8ff075ab551a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 2.3.2:** \n",
    "Continuing with the `df_weather_period` from last exercise, do the following:\n",
    "> 1. Convert the `area` column to a categorical variable. \n",
    "> 2. Transform the `obs_value` column from a continuous to a categorical variable by partitioning it into `3` intervals. The first interval should contain observations with values of `obs_value` up to the 10% quantile. The second interval should contain observations with values of `obs_value` up to the 90% quantile. The third interval should contain the rest of the observations. Call this new column for `obs_value_cat`.  This can be done using the `pd.qcut()` method.\n",
    "> 3. Make another column with  `obs_value` as a categorical variable but this time label the 3 intervals as `[\"cold\", \"medium\", \"hot\"]`. This can be done by specifying the `labels` parameter in the `pd.qcut()` method of pandas. Call this new column for `obs_value_cat_labeled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a0243b6c65b39af72e8d1efead106e8",
     "grade": false,
     "grade_id": "problem_232",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_weather_period['area'] = pd.Categorical(df_weather_period.area)\n",
    "\n",
    "df_weather_period['obs_value_cat'] = pd.qcut(df_weather_period['obs_value'], [0,0.1,0.9,1])\n",
    "\n",
    "df_weather_period['obs_value_cat_labeled'] = pd.qcut(df_weather_period['obs_value'], [0,0.1,0.9,1], labels=['cold', 'medium', 'hot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ea686468a1612c1453d6013671443b9",
     "grade": true,
     "grade_id": "problem_232_tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0e767d450ff726563ebe1bdb729215f",
     "grade": false,
     "grade_id": "cell-77eabac0ab0cbce5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f6944ea47bde40b92ba269f19d16439",
     "grade": false,
     "grade_id": "cell-4975a2e1ab215936",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.1:** Compute the mean and median maximum daily temperature for each month-year-station pair on the dataframe `df_weather_period` from last exercise by using the _split-apply-combine_ procedure. Store the results in new columns `tmax_mean` and `tmax_median`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce92e895d0a63283094fe6c661cb5b66",
     "grade": false,
     "grade_id": "problem_331",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_weather_period['tmax_mean'] = df_weather_period.groupby(['month','year','station'])['obs_value'].transform('mean')\n",
    "\n",
    "df_weather_period['tmax_median'] = df_weather_period.groupby(['month','year','station'])['obs_value'].transform('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b200933c81339b97661155bc29d76cef",
     "grade": true,
     "grade_id": "problem_331_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e4d376c6fe462ddc61d2839b982968b",
     "grade": false,
     "grade_id": "cell-7e77713f98953bac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.2:** Plot the monthly max,min, mean, first and third quartiles for maximum temperature for the station with ID _'CA006110549'_ from `df_weather_period`.\n",
    "\n",
    "> *Hint*: the method `describe` computes all these measures. Try to make your plot look like the one below. \n",
    "\n",
    "<img src=\"station_data_plot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca1afdbf1edee8beacbfc1e95d6ac2e4",
     "grade": true,
     "grade_id": "problem_332_tests",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.2)\n",
    "\n",
    "df_plot = df_weather_period.loc[df_weather_period['station'] == 'CA006110549'].groupby('month')\n",
    "df_plot = df_plot.obs_value.describe()\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.plot(df_plot.index, df_plot['mean'], color='red', label='Mean')\n",
    "plt.plot(df_plot.index, df_plot['min'], color='blue', label='Min', linestyle=':', dashes=(2, 20))\n",
    "plt.plot(df_plot.index, df_plot['max'], color='blue', label='Max', linestyle=':', dashes=(2, 20))\n",
    "plt.plot(df_plot.index, df_plot['25%'], color='blue', label='25% quantile', linestyle='--')\n",
    "plt.plot(df_plot.index, df_plot['75%'], color='blue', label='75% quantile',linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "ax.set_title('Temperature measures for weatherstation CA006110549')\n",
    "ax.set_ylabel('Temperature ($^\\circ$C)')\n",
    "ax.set_xlabel('Month')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf7d78380538170cdb3f8da6d976c6cd",
     "grade": false,
     "grade_id": "cell-539af69a1ea23069",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3: (MODIFIED FOR ASSIGNMENT 1)** We want to use the location data of the weather stations and merge this onto `df_weather_period`. The file with station location data is called  `ghcnd-stations.txt` and is stored in the `data.zip` file. Therefore, by Ex. 2.X.2, it should now be located in the `data` folder of this directory. `pandas` has a function named [`read_fwf`](https://pandas.pydata.org/docs/reference/api/pandas.read_fwf.html) which can be used to read a txt file with a fixed width format (each variable spans a fixed amount of columns). The function is neat and can infer how many columns each variable spans automatically (if the `infer_nrows` parameter is set properly). One can also manually set the `colspecs` parameter equal to a list of tuples containing the fixed-width intervals that the variables span. In the following exercise we will use some extra time and do the job manually to practice our txt file and string skills. Specifically, we will extract the list of tuples with fixed-widht information together with the column names and datatypes from the `ghcnd-stations-column-metadata.txt` file (also included in the `data.zip` file). \n",
    "\n",
    "> The `ghcnd-stations-column-metadata.txt` file looks like this: \n",
    "\n",
    "```\n",
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSN FLAG     73-75   Character\n",
    "HCN/CRN FLAG 77-79   Character\n",
    "WMO ID       81-85   Character\n",
    "------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4d926a239ad69a32e6ddcb443ef42a0",
     "grade": false,
     "grade_id": "cell-6a3113e42875692a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.1:** Read the `ghcnd-stations-column-metadata.txt` using the `with` keyword, see [here](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files), and store it in a variable called `column_metadata`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be143fcf053d269b2048157e33dc225c",
     "grade": false,
     "grade_id": "3331",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open(fp_data / 'ghcnd-stations-column-metadata.txt', 'r') as f:\n",
    "    column_metadata = f.read()\n",
    "    #print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1406fd53b4dd29083588279108f8b861",
     "grade": true,
     "grade_id": "3331-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d348ada2f5d006ebbb11b024f7139bc",
     "grade": false,
     "grade_id": "cell-9c66cca32bfbef31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.2:** Split `column_metadata` into a list of strings by applying the method `split` with the proper argument. Subset the resulting list and extract all lines from index `3` to `12` (non-inclusive) of the variable. Store the final list in a variable named `lines`. Inspect the result to make sure the relevant rows of the txt file has been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID            1-11   Character',\n",
       " 'LATITUDE     13-20   Real',\n",
       " 'LONGITUDE    22-30   Real',\n",
       " 'ELEVATION    32-37   Real',\n",
       " 'STATE        39-40   Character',\n",
       " 'NAME         42-71   Character',\n",
       " 'GSN FLAG     73-75   Character',\n",
       " 'HCN/CRN FLAG 77-79   Character',\n",
       " 'WMO ID       81-85   Character']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = column_metadata.split('\\n')\n",
    "lines = lines[3:12]\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a01223ced1b81abc7bb54537a1923fd3",
     "grade": true,
     "grade_id": "3332-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a2d97eb10f8245bcb67a28a98be2d91",
     "grade": false,
     "grade_id": "cell-6d6084e723953822",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.3:** Inspecting each line of the `lines` variable we see that the information about the column widths are all located from index `13` up and including index `17`. Finish the `get_colspecs` function below to extract the fixed width information from the `lines` variable by completing the steps below:\n",
    "1. Use a list comprehension to loop through each line of the file\n",
    "2. Index each line by the relevant indices written above\n",
    "3. Strip leading whitespace of each element (if necessary)\n",
    "\n",
    "> Finally, apply `get_colspecs` to the `lines` variable and store the result in a new variable called `colspecs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca2823ed345d0dd66e8782cd4da05ea2",
     "grade": false,
     "grade_id": "3333",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-11',\n",
       " '13-20',\n",
       " '22-30',\n",
       " '32-37',\n",
       " '39-40',\n",
       " '42-71',\n",
       " '73-75',\n",
       " '77-79',\n",
       " '81-85']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_colspecs(lines):\n",
    "    \"\"\"Extracts colspecs from `ghcnd-stations-column-metadata.txt`.\n",
    "    \n",
    "    Args:\n",
    "        lines (list[str]): \n",
    "            list of relevant rows from `ghcnd-stations-column-metadata.txt` \n",
    "    \n",
    "    Returns:\n",
    "        (list[str]): \n",
    "            list of extracted colspecs i.e. ['1-11', '13-20', ..., '81-85']\n",
    "    \"\"\"\n",
    "    colspec_idx_start = 13\n",
    "    colspec_idx_end = 17 + 1  # Including idx 17\n",
    "\n",
    "\n",
    "    # Insert missing line\n",
    "    colspecs = [line[colspec_idx_start:colspec_idx_end].strip() for line in lines]\n",
    "    \n",
    "    return colspecs\n",
    "\n",
    "\n",
    "colspecs = get_colspecs(lines)\n",
    "colspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44cd16c209eaf640a81923fb6c6ad3f1",
     "grade": true,
     "grade_id": "3333-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4c3b22be2785fd9a8e77273cb088905",
     "grade": false,
     "grade_id": "cell-6d9084804240b2d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.4:** Write a function named `get_colspec_pair` which takes as input a string variable named `colspec` and returns a tuple of integers. Specifically, the function should take a string similar to each element of `colspecs`, split this string by `-` and return a tuple of integers where\n",
    "1. The first integer should have `1` subtracted from it (Python is 0-indexed!)\n",
    "2. The second integer should stay as it is (the intervals provided to the pandas function `read_fwf` should be non-inclusive)\n",
    "> As an example, applying the function to `\"1-11\"` and `\"13-20\"` should yield the following results:\n",
    "\n",
    "```python\n",
    "print(get_colspec_pair(\"1-11\"))\n",
    "## output: (0, 11)\n",
    "\n",
    "print(get_colspec_pair(\"13-20\"))\n",
    "## output: (12, 20)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fa3780ae35b45977761ba2fb454216a",
     "grade": false,
     "grade_id": "3334",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 12)\n"
     ]
    }
   ],
   "source": [
    "def get_colspec_par(colspec):\n",
    "    \n",
    "    colspec = list(map(int, colspec.split('-')))\n",
    "    \n",
    "    colspec[0] = colspec[0] -1\n",
    "\n",
    "    colspec = tuple(map(int, colspec))\n",
    "\n",
    "    return colspec\n",
    "\n",
    "print(get_colspec_par(\"1-12\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8fee8d5b0d56d0ec1a96ed593c59a0d4",
     "grade": true,
     "grade_id": "3334-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ff7f17567aa4e2fbb44af24bdbcc986",
     "grade": false,
     "grade_id": "cell-cbaa1e1dca8e3015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.5:** Use the `get_colspec_pair` function in a list comprehension where you apply the function to each element in `colspecs`. Store the result in a variable named `colspec_pairs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3afbf83d84156e24b17573026dfa8248",
     "grade": false,
     "grade_id": "3335",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11),\n",
       " (12, 20),\n",
       " (21, 30),\n",
       " (31, 37),\n",
       " (38, 40),\n",
       " (41, 71),\n",
       " (72, 75),\n",
       " (76, 79),\n",
       " (80, 85)]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colspec_pairs = [get_colspec_par(i) for i in colspecs]\n",
    "colspec_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a12fae1b1b368de38f3c30a3e1cdc965",
     "grade": true,
     "grade_id": "3335-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2be26e96b4592760cfc20c18c6da9b1e",
     "grade": false,
     "grade_id": "cell-5535ad3d8666836b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.6:** Because the fixed width column information spans the interval from `13` up and including index `17`, we know that the entries from `0` to `13` (non-inclusive) are the column names and the entries from `18` to the end of each line are the data types. Write two functions named `get_column_names` and `get_column_dtypes` which return a list of column names and a list of the data types of the columns, respectively. Remember to strip all redundant whitespace using the string method `strip`. Apply the function `get_column_names` to the `lines` variable and store the output in a variable named `column_names`. Likewise, apply the function `get_column_dtypes` to the `lines` variable and store the output in a variable named `column_dtypes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f19450632f6c6d1d4d948405258b3f5c",
     "grade": false,
     "grade_id": "3336",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_column_names(colspecs):\n",
    "\n",
    "    \"\"\"Extracts colspecs from `ghcnd-stations-column-metadata.txt`.\n",
    "    \n",
    "    Args:\n",
    "        lines (list[str]): \n",
    "            list of relevant rows from `ghcnd-stations-column-metadata.txt` \n",
    "    \n",
    "    Returns:\n",
    "        (list[str]): \n",
    "            list of extracted colspecs\n",
    "    \"\"\"\n",
    "    colspec_idx_start = 0\n",
    "    colspec_idx_end = 13  # not including 13\n",
    "\n",
    "\n",
    "    # Insert missing line\n",
    "    colspecs = [line[colspec_idx_start:colspec_idx_end].strip() for line in lines]\n",
    "    \n",
    "    return colspecs\n",
    "\n",
    "def get_column_dtypes(colspecs):\n",
    "\n",
    "    \"\"\"Extracts colspecs from `ghcnd-stations-column-metadata.txt`.\n",
    "    \n",
    "    Args:\n",
    "        lines (list[str]): \n",
    "            list of relevant rows from `ghcnd-stations-column-metadata.txt` \n",
    "    \n",
    "    Returns:\n",
    "        (list[str]): \n",
    "            list of extracted colspecs\n",
    "    \"\"\"\n",
    "    colspec_idx_start = 18\n",
    "    colspec_idx_end = 1000\n",
    "\n",
    "\n",
    "    # Insert missing line\n",
    "    colspecs = [line[colspec_idx_start:colspec_idx_end].strip() for line in lines]\n",
    "    \n",
    "    return colspecs\n",
    "\n",
    "column_names = get_column_names(lines)\n",
    "\n",
    "column_dtypes = get_column_dtypes(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a4ad3c2aaa3e244b71d02a9c0d99303",
     "grade": true,
     "grade_id": "3336-tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42c0ef12aadd154859e3b41b8c7fabc1",
     "grade": false,
     "grade_id": "cell-6a9d81f37628d1ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.7:** Replace each `\"character\"` entry with `\"str\"` and each `\"real\"` entry with `\"float32\"` of the list `column_dtypes`. Store the result of this in the same variable `column_dtypes`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c43672a0ccde0a6cb5b628a91dfcd4d",
     "grade": false,
     "grade_id": "3337",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['str', 'float32', 'float32', 'float32', 'str', 'str', 'str', 'str', 'str']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_dtypes = [s.replace('Character', 'str').replace('Real', 'float32') for s in column_dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f3afd09a2073e36c109317f6e1f1b56e",
     "grade": true,
     "grade_id": "3337-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "517454579c02edf82638e4d2f6769d05",
     "grade": false,
     "grade_id": "cell-75834af9070629b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.8:** Load the `ghcnd-stations.txt` data using the `read_fwf` method of pandas setting the `names` parameter equal to `column_names` and the `colspecs` parameter equal to  `colspec_pairs`. Store the result in a variable named `locations`. Next, use the `astype` method on `locations` to set the dtypes of the columns. Use the `col_to_dtype` mapping below as input argument to `astype`. Finally, rename the `id` column to `station` and left-merge `locations` onto `df_weather_period`. Store the merged dataframe in the variable `df_weather_merged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f95d9887f5f7d1bb9294ef49c9ac05e3",
     "grade": true,
     "grade_id": "3338",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>datetime</th>\n",
       "      <th>obs_type</th>\n",
       "      <th>obs_value</th>\n",
       "      <th>area</th>\n",
       "      <th>datetime_dt</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>obs_value_cat</th>\n",
       "      <th>obs_value_cat_labeled</th>\n",
       "      <th>tmax_mean</th>\n",
       "      <th>tmax_median</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>GSN FLAG</th>\n",
       "      <th>HCN/CRN FLAG</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASN00066062</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>27.1</td>\n",
       "      <td>AS</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1870</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>25.800000</td>\n",
       "      <td>25.2</td>\n",
       "      <td>-33.8607</td>\n",
       "      <td>151.2050</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SYDNEY (OBSERVATORY HILL)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASN00074128</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>41.1</td>\n",
       "      <td>AS</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1870</td>\n",
       "      <td>(28.3, 47.8]</td>\n",
       "      <td>hot</td>\n",
       "      <td>34.620000</td>\n",
       "      <td>37.2</td>\n",
       "      <td>-35.5269</td>\n",
       "      <td>144.9520</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DENILIQUIN (WILKINSON ST)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASN00086071</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>23.3</td>\n",
       "      <td>AS</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1870</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>26.819355</td>\n",
       "      <td>24.3</td>\n",
       "      <td>-37.8075</td>\n",
       "      <td>144.9700</td>\n",
       "      <td>31.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MELBOURNE REGIONAL OFFICE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASN00090015</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>20.0</td>\n",
       "      <td>AS</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1870</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>23.906452</td>\n",
       "      <td>21.7</td>\n",
       "      <td>-38.8556</td>\n",
       "      <td>143.5128</td>\n",
       "      <td>82.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAPE OTWAY LIGHTHOUSE</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94842.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AU000005901</td>\n",
       "      <td>18700101</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>AU</td>\n",
       "      <td>1870-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1870</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>1.083871</td>\n",
       "      <td>0.9</td>\n",
       "      <td>48.2331</td>\n",
       "      <td>16.3500</td>\n",
       "      <td>199.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WIEN</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11035.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131815</th>\n",
       "      <td>USW00013724</td>\n",
       "      <td>18751231</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>9.4</td>\n",
       "      <td>US</td>\n",
       "      <td>1875-12-31</td>\n",
       "      <td>12</td>\n",
       "      <td>1875</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>5.919355</td>\n",
       "      <td>6.1</td>\n",
       "      <td>39.3792</td>\n",
       "      <td>-74.4242</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NJ</td>\n",
       "      <td>ATLANTIC CITY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131816</th>\n",
       "      <td>USW00024274</td>\n",
       "      <td>18751231</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>12.2</td>\n",
       "      <td>US</td>\n",
       "      <td>1875-12-31</td>\n",
       "      <td>12</td>\n",
       "      <td>1875</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>11.490323</td>\n",
       "      <td>12.2</td>\n",
       "      <td>45.5333</td>\n",
       "      <td>-122.6667</td>\n",
       "      <td>9.1</td>\n",
       "      <td>OR</td>\n",
       "      <td>PORTLAND RFC CITY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131817</th>\n",
       "      <td>USW00093820</td>\n",
       "      <td>18751231</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>21.7</td>\n",
       "      <td>US</td>\n",
       "      <td>1875-12-31</td>\n",
       "      <td>12</td>\n",
       "      <td>1875</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>9.703226</td>\n",
       "      <td>10.6</td>\n",
       "      <td>38.0408</td>\n",
       "      <td>-84.6058</td>\n",
       "      <td>298.7</td>\n",
       "      <td>KY</td>\n",
       "      <td>LEXINGTON BLUEGRASS AP</td>\n",
       "      <td>GSN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131818</th>\n",
       "      <td>USW00093852</td>\n",
       "      <td>18751231</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>25.6</td>\n",
       "      <td>US</td>\n",
       "      <td>1875-12-31</td>\n",
       "      <td>12</td>\n",
       "      <td>1875</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>19.858065</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.3333</td>\n",
       "      <td>-81.6500</td>\n",
       "      <td>25.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>JACKSONVILLE WB CITY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131819</th>\n",
       "      <td>USW00094728</td>\n",
       "      <td>18751231</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>7.2</td>\n",
       "      <td>US</td>\n",
       "      <td>1875-12-31</td>\n",
       "      <td>12</td>\n",
       "      <td>1875</td>\n",
       "      <td>(-1.1, 28.3]</td>\n",
       "      <td>medium</td>\n",
       "      <td>3.890323</td>\n",
       "      <td>3.9</td>\n",
       "      <td>40.7789</td>\n",
       "      <td>-73.9692</td>\n",
       "      <td>39.6</td>\n",
       "      <td>NY</td>\n",
       "      <td>NEW YORK CNTRL PK TWR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HCN</td>\n",
       "      <td>72506.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131820 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            station  datetime obs_type  obs_value area datetime_dt  month  \\\n",
       "0       ASN00066062  18700101     TMAX       27.1   AS  1870-01-01      1   \n",
       "1       ASN00074128  18700101     TMAX       41.1   AS  1870-01-01      1   \n",
       "2       ASN00086071  18700101     TMAX       23.3   AS  1870-01-01      1   \n",
       "3       ASN00090015  18700101     TMAX       20.0   AS  1870-01-01      1   \n",
       "4       AU000005901  18700101     TMAX       -0.9   AU  1870-01-01      1   \n",
       "...             ...       ...      ...        ...  ...         ...    ...   \n",
       "131815  USW00013724  18751231     TMAX        9.4   US  1875-12-31     12   \n",
       "131816  USW00024274  18751231     TMAX       12.2   US  1875-12-31     12   \n",
       "131817  USW00093820  18751231     TMAX       21.7   US  1875-12-31     12   \n",
       "131818  USW00093852  18751231     TMAX       25.6   US  1875-12-31     12   \n",
       "131819  USW00094728  18751231     TMAX        7.2   US  1875-12-31     12   \n",
       "\n",
       "        year obs_value_cat obs_value_cat_labeled  tmax_mean  tmax_median  \\\n",
       "0       1870  (-1.1, 28.3]                medium  25.800000         25.2   \n",
       "1       1870  (28.3, 47.8]                   hot  34.620000         37.2   \n",
       "2       1870  (-1.1, 28.3]                medium  26.819355         24.3   \n",
       "3       1870  (-1.1, 28.3]                medium  23.906452         21.7   \n",
       "4       1870  (-1.1, 28.3]                medium   1.083871          0.9   \n",
       "...      ...           ...                   ...        ...          ...   \n",
       "131815  1875  (-1.1, 28.3]                medium   5.919355          6.1   \n",
       "131816  1875  (-1.1, 28.3]                medium  11.490323         12.2   \n",
       "131817  1875  (-1.1, 28.3]                medium   9.703226         10.6   \n",
       "131818  1875  (-1.1, 28.3]                medium  19.858065         20.0   \n",
       "131819  1875  (-1.1, 28.3]                medium   3.890323          3.9   \n",
       "\n",
       "        LATITUDE  LONGITUDE  ELEVATION STATE                       NAME  \\\n",
       "0       -33.8607   151.2050       39.0   NaN  SYDNEY (OBSERVATORY HILL)   \n",
       "1       -35.5269   144.9520       93.0   NaN  DENILIQUIN (WILKINSON ST)   \n",
       "2       -37.8075   144.9700       31.2   NaN  MELBOURNE REGIONAL OFFICE   \n",
       "3       -38.8556   143.5128       82.0   NaN      CAPE OTWAY LIGHTHOUSE   \n",
       "4        48.2331    16.3500      199.0   NaN                       WIEN   \n",
       "...          ...        ...        ...   ...                        ...   \n",
       "131815   39.3792   -74.4242        3.0    NJ              ATLANTIC CITY   \n",
       "131816   45.5333  -122.6667        9.1    OR          PORTLAND RFC CITY   \n",
       "131817   38.0408   -84.6058      298.7    KY     LEXINGTON BLUEGRASS AP   \n",
       "131818   30.3333   -81.6500       25.0    FL       JACKSONVILLE WB CITY   \n",
       "131819   40.7789   -73.9692       39.6    NY      NEW YORK CNTRL PK TWR   \n",
       "\n",
       "       GSN FLAG HCN/CRN FLAG   WMO ID  \n",
       "0           NaN          NaN  94768.0  \n",
       "1           NaN          NaN      NaN  \n",
       "2           NaN          NaN  94868.0  \n",
       "3           GSN          NaN  94842.0  \n",
       "4           GSN          NaN  11035.0  \n",
       "...         ...          ...      ...  \n",
       "131815      NaN          HCN      NaN  \n",
       "131816      NaN          NaN      NaN  \n",
       "131817      GSN          NaN  72422.0  \n",
       "131818      NaN          NaN      NaN  \n",
       "131819      NaN          HCN  72506.0  \n",
       "\n",
       "[131820 rows x 20 columns]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_dtype = dict(zip(column_names, column_dtypes))\n",
    "\n",
    "locations = pd.read_fwf(fp_data / 'ghcnd-stations.txt', names=column_names, colspecs=colspec_pairs)\n",
    "\n",
    "locations.astype(col_to_dtype)\n",
    "\n",
    "locations.rename({'ID': 'station'}, axis=1, inplace=True)\n",
    "\n",
    "df_weather_merged = pd.merge(df_weather_period, locations, how='left')\n",
    "\n",
    "df_weather_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9a4a76770858c976a5b06ed3ae844c1",
     "grade": false,
     "grade_id": "cell-5ba4eb25c926ef77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.9:** Subset `df_weather_period` by all weather stations in Ontario (all stations in Ontario have `state == \"ON\"`) and store the resulting DataFrame in `df_ontario`. Compute the average `obs_value` for each `station`. Store the result in a dictionary named `avg_obs_value_ontario` with the keys being the station names and the values the average `obs_value`. Finally, subset the `locations` dataframe by the querying all stations contained in the keys of `avg_obs_value_ontario`. Store the result in `locations_ontario`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "281bdf9651d8963912d805659386a298",
     "grade": false,
     "grade_id": "3339",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>obs_value</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>tmax_mean</th>\n",
       "      <th>tmax_median</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>WMO ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CA006094450</th>\n",
       "      <td>1.873640e+07</td>\n",
       "      <td>10.095722</td>\n",
       "      <td>6.612763</td>\n",
       "      <td>1873.572154</td>\n",
       "      <td>10.095722</td>\n",
       "      <td>10.191806</td>\n",
       "      <td>45.9333</td>\n",
       "      <td>-81.9000</td>\n",
       "      <td>188.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006100969</th>\n",
       "      <td>1.873749e+07</td>\n",
       "      <td>11.461325</td>\n",
       "      <td>6.888290</td>\n",
       "      <td>1873.678737</td>\n",
       "      <td>11.461325</td>\n",
       "      <td>11.677042</td>\n",
       "      <td>44.6000</td>\n",
       "      <td>-75.7000</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006101872</th>\n",
       "      <td>1.871862e+07</td>\n",
       "      <td>10.968613</td>\n",
       "      <td>6.538287</td>\n",
       "      <td>1871.795299</td>\n",
       "      <td>10.968613</td>\n",
       "      <td>11.149280</td>\n",
       "      <td>45.0167</td>\n",
       "      <td>-74.7333</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006104185</th>\n",
       "      <td>1.873783e+07</td>\n",
       "      <td>11.532682</td>\n",
       "      <td>6.953088</td>\n",
       "      <td>1873.712275</td>\n",
       "      <td>11.532682</td>\n",
       "      <td>11.713604</td>\n",
       "      <td>44.2500</td>\n",
       "      <td>-76.5000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006105887</th>\n",
       "      <td>1.873666e+07</td>\n",
       "      <td>11.323258</td>\n",
       "      <td>6.763756</td>\n",
       "      <td>1873.597212</td>\n",
       "      <td>11.323258</td>\n",
       "      <td>11.355979</td>\n",
       "      <td>45.4000</td>\n",
       "      <td>-75.7167</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006106362</th>\n",
       "      <td>1.871299e+07</td>\n",
       "      <td>9.999413</td>\n",
       "      <td>6.020117</td>\n",
       "      <td>1871.237217</td>\n",
       "      <td>9.999413</td>\n",
       "      <td>10.281308</td>\n",
       "      <td>45.8333</td>\n",
       "      <td>-77.1500</td>\n",
       "      <td>125.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006110549</th>\n",
       "      <td>1.872056e+07</td>\n",
       "      <td>10.395920</td>\n",
       "      <td>6.049479</td>\n",
       "      <td>1871.993924</td>\n",
       "      <td>10.395920</td>\n",
       "      <td>10.741927</td>\n",
       "      <td>44.4000</td>\n",
       "      <td>-79.6833</td>\n",
       "      <td>229.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006113000</th>\n",
       "      <td>1.875103e+07</td>\n",
       "      <td>9.073611</td>\n",
       "      <td>10.125000</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>9.073611</td>\n",
       "      <td>9.481250</td>\n",
       "      <td>44.9167</td>\n",
       "      <td>-79.3667</td>\n",
       "      <td>248.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006116254</th>\n",
       "      <td>1.875117e+07</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>11.508197</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.788525</td>\n",
       "      <td>45.3333</td>\n",
       "      <td>-80.0000</td>\n",
       "      <td>194.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006122845</th>\n",
       "      <td>1.872427e+07</td>\n",
       "      <td>10.672918</td>\n",
       "      <td>6.253865</td>\n",
       "      <td>1872.362594</td>\n",
       "      <td>10.672918</td>\n",
       "      <td>10.656259</td>\n",
       "      <td>43.7500</td>\n",
       "      <td>-81.7000</td>\n",
       "      <td>220.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006124127</th>\n",
       "      <td>1.873826e+07</td>\n",
       "      <td>10.834385</td>\n",
       "      <td>6.911130</td>\n",
       "      <td>1873.754983</td>\n",
       "      <td>10.834385</td>\n",
       "      <td>10.799252</td>\n",
       "      <td>44.1667</td>\n",
       "      <td>-81.6167</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006127887</th>\n",
       "      <td>1.874455e+07</td>\n",
       "      <td>9.700379</td>\n",
       "      <td>6.910240</td>\n",
       "      <td>1874.384324</td>\n",
       "      <td>9.700379</td>\n",
       "      <td>9.588622</td>\n",
       "      <td>44.5000</td>\n",
       "      <td>-81.3667</td>\n",
       "      <td>186.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006136643</th>\n",
       "      <td>1.875098e+07</td>\n",
       "      <td>13.781871</td>\n",
       "      <td>9.684211</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>13.781871</td>\n",
       "      <td>13.933333</td>\n",
       "      <td>42.7833</td>\n",
       "      <td>-80.2167</td>\n",
       "      <td>186.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006136694</th>\n",
       "      <td>1.874674e+07</td>\n",
       "      <td>13.453859</td>\n",
       "      <td>7.321839</td>\n",
       "      <td>1874.599343</td>\n",
       "      <td>13.453859</td>\n",
       "      <td>13.534975</td>\n",
       "      <td>42.6667</td>\n",
       "      <td>-81.2167</td>\n",
       "      <td>183.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006137735</th>\n",
       "      <td>1.871710e+07</td>\n",
       "      <td>13.349282</td>\n",
       "      <td>6.262283</td>\n",
       "      <td>1871.645503</td>\n",
       "      <td>13.349282</td>\n",
       "      <td>13.353439</td>\n",
       "      <td>42.8667</td>\n",
       "      <td>-80.3333</td>\n",
       "      <td>223.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006139445</th>\n",
       "      <td>1.873914e+07</td>\n",
       "      <td>11.078921</td>\n",
       "      <td>6.874368</td>\n",
       "      <td>1873.844013</td>\n",
       "      <td>11.078921</td>\n",
       "      <td>11.190978</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>-79.2667</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006139520</th>\n",
       "      <td>1.872496e+07</td>\n",
       "      <td>13.537842</td>\n",
       "      <td>6.555312</td>\n",
       "      <td>1872.429354</td>\n",
       "      <td>13.537842</td>\n",
       "      <td>13.401314</td>\n",
       "      <td>42.3333</td>\n",
       "      <td>-82.9333</td>\n",
       "      <td>188.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006142993</th>\n",
       "      <td>1.875117e+07</td>\n",
       "      <td>1.370909</td>\n",
       "      <td>11.563636</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>1.370909</td>\n",
       "      <td>1.841818</td>\n",
       "      <td>43.2000</td>\n",
       "      <td>-81.3333</td>\n",
       "      <td>316.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006143780</th>\n",
       "      <td>1.873798e+07</td>\n",
       "      <td>12.298817</td>\n",
       "      <td>6.952681</td>\n",
       "      <td>1873.727129</td>\n",
       "      <td>12.298817</td>\n",
       "      <td>12.332177</td>\n",
       "      <td>43.0500</td>\n",
       "      <td>-80.8833</td>\n",
       "      <td>267.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006144470</th>\n",
       "      <td>1.872658e+07</td>\n",
       "      <td>14.661713</td>\n",
       "      <td>7.400350</td>\n",
       "      <td>1872.582168</td>\n",
       "      <td>14.661713</td>\n",
       "      <td>14.686713</td>\n",
       "      <td>42.9833</td>\n",
       "      <td>-81.2000</td>\n",
       "      <td>246.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006148100</th>\n",
       "      <td>1.871710e+07</td>\n",
       "      <td>10.051506</td>\n",
       "      <td>6.259789</td>\n",
       "      <td>1871.645331</td>\n",
       "      <td>10.051506</td>\n",
       "      <td>10.195783</td>\n",
       "      <td>43.3833</td>\n",
       "      <td>-81.0000</td>\n",
       "      <td>363.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006149625</th>\n",
       "      <td>1.873664e+07</td>\n",
       "      <td>11.377228</td>\n",
       "      <td>6.466108</td>\n",
       "      <td>1873.597867</td>\n",
       "      <td>11.377228</td>\n",
       "      <td>11.500838</td>\n",
       "      <td>43.1333</td>\n",
       "      <td>-80.7667</td>\n",
       "      <td>282.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006150689</th>\n",
       "      <td>1.871710e+07</td>\n",
       "      <td>11.162528</td>\n",
       "      <td>6.260346</td>\n",
       "      <td>1871.645598</td>\n",
       "      <td>11.162528</td>\n",
       "      <td>11.345523</td>\n",
       "      <td>44.1500</td>\n",
       "      <td>-77.4000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006153192</th>\n",
       "      <td>1.871735e+07</td>\n",
       "      <td>12.731416</td>\n",
       "      <td>6.231488</td>\n",
       "      <td>1871.671459</td>\n",
       "      <td>12.731416</td>\n",
       "      <td>12.763551</td>\n",
       "      <td>43.2667</td>\n",
       "      <td>-79.9000</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006155616</th>\n",
       "      <td>1.875076e+07</td>\n",
       "      <td>12.035252</td>\n",
       "      <td>7.478417</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>12.035252</td>\n",
       "      <td>12.298201</td>\n",
       "      <td>44.0667</td>\n",
       "      <td>-79.4333</td>\n",
       "      <td>282.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006158350</th>\n",
       "      <td>1.872567e+07</td>\n",
       "      <td>11.105523</td>\n",
       "      <td>6.523962</td>\n",
       "      <td>1872.499772</td>\n",
       "      <td>11.105523</td>\n",
       "      <td>11.230945</td>\n",
       "      <td>43.6667</td>\n",
       "      <td>-79.4000</td>\n",
       "      <td>113.0</td>\n",
       "      <td>71266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006165716</th>\n",
       "      <td>1.875112e+07</td>\n",
       "      <td>3.312791</td>\n",
       "      <td>11.069767</td>\n",
       "      <td>1875.000000</td>\n",
       "      <td>3.312791</td>\n",
       "      <td>3.283721</td>\n",
       "      <td>44.3833</td>\n",
       "      <td>-77.9667</td>\n",
       "      <td>213.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA006166416</th>\n",
       "      <td>1.871712e+07</td>\n",
       "      <td>12.022710</td>\n",
       "      <td>6.290689</td>\n",
       "      <td>1871.647237</td>\n",
       "      <td>12.022710</td>\n",
       "      <td>12.019455</td>\n",
       "      <td>44.2833</td>\n",
       "      <td>-78.3167</td>\n",
       "      <td>194.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 datetime  obs_value      month         year  tmax_mean  \\\n",
       "station                                                                   \n",
       "CA006094450  1.873640e+07  10.095722   6.612763  1873.572154  10.095722   \n",
       "CA006100969  1.873749e+07  11.461325   6.888290  1873.678737  11.461325   \n",
       "CA006101872  1.871862e+07  10.968613   6.538287  1871.795299  10.968613   \n",
       "CA006104185  1.873783e+07  11.532682   6.953088  1873.712275  11.532682   \n",
       "CA006105887  1.873666e+07  11.323258   6.763756  1873.597212  11.323258   \n",
       "CA006106362  1.871299e+07   9.999413   6.020117  1871.237217   9.999413   \n",
       "CA006110549  1.872056e+07  10.395920   6.049479  1871.993924  10.395920   \n",
       "CA006113000  1.875103e+07   9.073611  10.125000  1875.000000   9.073611   \n",
       "CA006116254  1.875117e+07   0.700000  11.508197  1875.000000   0.700000   \n",
       "CA006122845  1.872427e+07  10.672918   6.253865  1872.362594  10.672918   \n",
       "CA006124127  1.873826e+07  10.834385   6.911130  1873.754983  10.834385   \n",
       "CA006127887  1.874455e+07   9.700379   6.910240  1874.384324   9.700379   \n",
       "CA006136643  1.875098e+07  13.781871   9.684211  1875.000000  13.781871   \n",
       "CA006136694  1.874674e+07  13.453859   7.321839  1874.599343  13.453859   \n",
       "CA006137735  1.871710e+07  13.349282   6.262283  1871.645503  13.349282   \n",
       "CA006139445  1.873914e+07  11.078921   6.874368  1873.844013  11.078921   \n",
       "CA006139520  1.872496e+07  13.537842   6.555312  1872.429354  13.537842   \n",
       "CA006142993  1.875117e+07   1.370909  11.563636  1875.000000   1.370909   \n",
       "CA006143780  1.873798e+07  12.298817   6.952681  1873.727129  12.298817   \n",
       "CA006144470  1.872658e+07  14.661713   7.400350  1872.582168  14.661713   \n",
       "CA006148100  1.871710e+07  10.051506   6.259789  1871.645331  10.051506   \n",
       "CA006149625  1.873664e+07  11.377228   6.466108  1873.597867  11.377228   \n",
       "CA006150689  1.871710e+07  11.162528   6.260346  1871.645598  11.162528   \n",
       "CA006153192  1.871735e+07  12.731416   6.231488  1871.671459  12.731416   \n",
       "CA006155616  1.875076e+07  12.035252   7.478417  1875.000000  12.035252   \n",
       "CA006158350  1.872567e+07  11.105523   6.523962  1872.499772  11.105523   \n",
       "CA006165716  1.875112e+07   3.312791  11.069767  1875.000000   3.312791   \n",
       "CA006166416  1.871712e+07  12.022710   6.290689  1871.647237  12.022710   \n",
       "\n",
       "             tmax_median  LATITUDE  LONGITUDE  ELEVATION   WMO ID  \n",
       "station                                                            \n",
       "CA006094450    10.191806   45.9333   -81.9000      188.0      NaN  \n",
       "CA006100969    11.677042   44.6000   -75.7000       91.0      NaN  \n",
       "CA006101872    11.149280   45.0167   -74.7333       53.0      NaN  \n",
       "CA006104185    11.713604   44.2500   -76.5000      104.0      NaN  \n",
       "CA006105887    11.355979   45.4000   -75.7167       72.0      NaN  \n",
       "CA006106362    10.281308   45.8333   -77.1500      125.0      NaN  \n",
       "CA006110549    10.741927   44.4000   -79.6833      229.0      NaN  \n",
       "CA006113000     9.481250   44.9167   -79.3667      248.0      NaN  \n",
       "CA006116254     1.788525   45.3333   -80.0000      194.0      NaN  \n",
       "CA006122845    10.656259   43.7500   -81.7000      220.0      NaN  \n",
       "CA006124127    10.799252   44.1667   -81.6167      200.0      NaN  \n",
       "CA006127887     9.588622   44.5000   -81.3667      186.0      NaN  \n",
       "CA006136643    13.933333   42.7833   -80.2167      186.0      NaN  \n",
       "CA006136694    13.534975   42.6667   -81.2167      183.0      NaN  \n",
       "CA006137735    13.353439   42.8667   -80.3333      223.0      NaN  \n",
       "CA006139445    11.190978   43.0000   -79.2667      175.0      NaN  \n",
       "CA006139520    13.401314   42.3333   -82.9333      188.0      NaN  \n",
       "CA006142993     1.841818   43.2000   -81.3333      316.0      NaN  \n",
       "CA006143780    12.332177   43.0500   -80.8833      267.0      NaN  \n",
       "CA006144470    14.686713   42.9833   -81.2000      246.0      NaN  \n",
       "CA006148100    10.195783   43.3833   -81.0000      363.0      NaN  \n",
       "CA006149625    11.500838   43.1333   -80.7667      282.0      NaN  \n",
       "CA006150689    11.345523   44.1500   -77.4000       76.0      NaN  \n",
       "CA006153192    12.763551   43.2667   -79.9000       92.0      NaN  \n",
       "CA006155616    12.298201   44.0667   -79.4333      282.0      NaN  \n",
       "CA006158350    11.230945   43.6667   -79.4000      113.0  71266.0  \n",
       "CA006165716     3.283721   44.3833   -77.9667      213.0      NaN  \n",
       "CA006166416    12.019455   44.2833   -78.3167      194.0      NaN  "
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ontatio = df_weather_merged.loc[df_weather_merged['STATE'] == 'ON']\n",
    "\n",
    "avg_obs_value_ontatio = df_ontatio.groupby('station').mean(['obs_value'])\n",
    "\n",
    "avg_obs_value_ontatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0c4d941c8aa454f62466c826d87602a",
     "grade": true,
     "grade_id": "3339-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c3a1d9147bc363623dcefcb7c27d5d5",
     "grade": false,
     "grade_id": "cell-8d4c53302d51c9db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.10 (OPTIONAL)**: The following exercise does not count towards the grade of this assignment. Let's try to plot the stations for Ontario on a map of Ontario. We'll use the [`folium`](http://python-visualization.github.io/folium/) package to do this. This package is not pre-installed with `anaconda`. Run the cell below to install the package or open up your terminal, activate your preferred conda environment and type `!pip install folium`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8765bee40dfff161e74121cdcf5fcb42",
     "grade": false,
     "grade_id": "cell-444d95c01e37753f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 3.3.3.10 (continued)**:\n",
    "> We want to plot the stations in `locations_ontario` on top of a map of Ontario. To do this, we need to create a `folium.Marker` for each station and place this on the folium map named `m` in the cell below starting with `import folium`. To accomplish this do the following:\n",
    "- Iterate through the `zipper` defined in the cell below using a list comprehension and apply the `get_marker` function at each iteration. \n",
    "    - The `zipper` object yields a tuple of 4 values in each iteration. \n",
    "- The `avg_temp` argument of `get_marker` should take the value of each given station from the `avg_obs_value_ontario` dictionary created in the previous exercise. If the loop variable corresponding to `locations_ontario.station` is named `station_id` the value can be computed by subsetting the dictionary as  `avg_obs_value_ontario[station_id]`.\n",
    "- Store the result in a variable named `markers_ontario`. The result should be a list of `folium.Markers` for each of the stations.\n",
    "\n",
    "The resulting plot should be an interactive plot similar to the one in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4cd34d87209cc1d736bf17963ca1278",
     "grade": false,
     "grade_id": "cell-09786db74bccea07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Resulting folium plot\n",
    "from IPython.display import Image\n",
    "Image(filename='ontario-example-plot.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4402f4bff9fb1f9e77ba879131dfdef4",
     "grade": true,
     "grade_id": "33310",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "\n",
    "def get_marker(lat, lon, station_name, avg_obs_value, icon='cloud', color=\"blue\"):\n",
    "    \"\"\"Creates a `folumn.Marker` for a given station\n",
    "    \n",
    "    Args:\n",
    "        (lat): lattitude of station\n",
    "        (lon): longitude of station\n",
    "        (station_name): name of station\n",
    "        (avg_obs_value): avg. obs_value for given station\n",
    "        \n",
    "    Returns:\n",
    "        (folium.Marker): object to be added to a folium map\n",
    "    \"\"\"\n",
    "    popup = \"\\n\".join([station_name, f\"Avg. obs_value: {avg_obs_value:.2f}\"])\n",
    "    marker = folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup,\n",
    "        icon=folium.Icon(icon=icon, color=color, )\n",
    "    )\n",
    "    return marker\n",
    "\n",
    "\n",
    "# Create folium map centered on Ontario\n",
    "# COORDS_ONTARIO = (51.730703, -86.938937)\n",
    "COORDS_ONTARIO = (43.40168574192175, -80.33021323830818)\n",
    "m = folium.Map(location=COORDS_ONTARIO, zoom_start=6)\n",
    "\n",
    "# Zipper object to iterate through\n",
    "zipper = zip(\n",
    "    locations_ontario.latitude,\n",
    "    locations_ontario.longitude,\n",
    "    locations_ontario.name,\n",
    "    locations_ontario.station   \n",
    ")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# add weather station markers to map \n",
    "for station_marker in markers_ontario:  \n",
    "    station_marker.add_to(m)\n",
    "m  # Display map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eaf086f4f6724e090ef66a74eff4517e",
     "grade": false,
     "grade_id": "cell-422d30deb292b4c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Problems from Exercise Set 4:\n",
    "\n",
    "> **Ex. 4.3.5 (sligthly modified):** This exercise consists of a set of small subelements: \n",
    ">\n",
    "> 0. Show the first five rows of the titanic dataset. What information is in the dataset?\n",
    "> 1. Use a barplot to show the probability of survival for men and women within each passenger class. \n",
    "> 2. Can you make a boxplot showing the same information (why/why not?). \n",
    "> 3. Show a boxplot for the fare-prices within each passenger class. \n",
    "> 4. Create a new subfolder as done in Ex. 2.X.1 this time named `figs`. Use the same approach as in Ex. 2.X.1 and store the `Path` object in a variable named `fp_figs`. \n",
    "> 5. Combine the two of the figures you created above into a two-panel figure and save it on your computer in the `figs` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e46d24e4bd08f8870982dd932ddd15f1",
     "grade": true,
     "grade_id": "problem_435",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.3.6:** Using the iris flower dataset, draw a scatterplot of sepal length and petal length. Include a second order polynomial fitted to the data. Add a title to the plot and rename the axis labels.\n",
    ">\n",
    "> _Write 3 sentences:_ Is this a meaningful way to display the data? What could we do differently?\n",
    ">\n",
    "> For a better understanding of the dataset this image might be useful:\n",
    "\n",
    "> <img src=\"example-iris-q436.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    ">\n",
    "> _Hint:_ Use the `.regplot` method from seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e41badd527517260b61cead987a91cf",
     "grade": true,
     "grade_id": "problem_436",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris.astype(str)\n",
    "\n",
    "sns.regplot(x = iris.sepal_length, y = iris.petal_length, scatter=True, order=2)\n",
    "plt.legend(labels=[\"Sepal\",\"Petal\"])\n",
    "plt.title(\"Scatter plot over lengths of Sepal and Petal\")\n",
    "plt.xlabel(\"Sepal length\")\n",
    "plt.ylabel(\"Petal length\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Is this a meaningful way to display the data? What could we do differently?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4330f62f04b07d60e818eb1893bbf82d",
     "grade": false,
     "grade_id": "cell-e6d0c56f1cf535c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "> **Ex. 4.3.7:** Use [pairplot with hue](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to create a figure that clearly shows how the different species vary across measurements in the iris dataset. Change the color palette and remove the shading from the density plots. _Bonus:_ Try to explain how the `diag_kws` argument works (_hint:_ [read here](https://stackoverflow.com/questions/1769403/understanding-kwargs-in-python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19e3feab810ee078ec29408d99334983",
     "grade": true,
     "grade_id": "problem_437",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(iris, height=2.3, hue=\"species\",\n",
    "        palette =\"rocket\",\n",
    "        diag_kws={'alpha':.0}, diag_kind = \"kde\")\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "diag_kws can remove the shading from the density plots\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
