{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISDS 2022 Notebook\n",
    "\n",
    "This notebook contains all relevant code for group 4 in the ISDS class of 2022. It belongs thereby to Nicolai, Mads, Elena and Emilie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting import and downloads\n",
    "# This section can be skipped if all relevant packages have been downloaded.\n",
    "\n",
    "%pip install selenium\n",
    "%pip install webdriver_manager\n",
    "%python -m nltk.downloader popular\n",
    "%pip install tensorflow\n",
    "%pip install spacy\n",
    "%pip install wordloud\n",
    "%python -m spacy download da_core_news_md\n",
    "%pip install keras\n",
    "%pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import time\n",
    "from queue import Empty\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import string\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import lemmy\n",
    "\n",
    "from function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stopwords and setting lemmatizer, seed\n",
    "with open('stopord.txt', encoding='utf8') as f:\n",
    "    stopord = f.read().splitlines()\n",
    "stop_word = nltk.corpus.stopwords.words('danish') + stopord\n",
    "\n",
    "nlp = spacy.load('da_core_news_md')\n",
    "lemmatizer = lemmy.load(\"da\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "seed = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobindex(tag, pages):\n",
    "    \"\"\"\n",
    "    This function seeks to scrap and find all link of relevant job postings\n",
    "    \"\"\"\n",
    "    flat_list = []\n",
    "    url_list = []\n",
    "    total_pages = range(pages)\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    for page in total_pages:\n",
    "        url = f\"https://www.jobindex.dk/jobsoegning?maxdate=20220101&mindate=20210101&page={page}&jobage=archive&q={tag}\"\n",
    "        r = requests.get(url, headers)\n",
    "        time.sleep(0.5)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\") \n",
    "        divs = soup.find_all(\"div\", class_=\"jobsearch-result\")\n",
    "        for item in divs:\n",
    "            try:\n",
    "                job_url = item.select_one('[data-click*=\"u=\"]:has(> b)')['data-click']\n",
    "                url_list.append(job_url)\n",
    "            except:\n",
    "                pass\n",
    "        for i in url_list:\n",
    "            link = 'http://www.jobindex.dk' + i\n",
    "            flat_list.append(link)\n",
    "    return flat_list\n",
    "\n",
    "def fetching_occupation(uri):\n",
    "    \"\"\"\n",
    "    This function fetch occupations for ESCO and append them in a dataframe\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    url = f'http://ec.europa.eu/esco/api/resource/occupation?uri={uri}&language=da'\n",
    "    response = requests.get(url, headers)\n",
    "    time.sleep(0.5)\n",
    "    result = response.json()\n",
    "    job_title = result['title']\n",
    "    joblist = []\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            essential_skill = result['_links']['hasEssentialSkill'][i]['title']\n",
    "            optional_skill = result['_links']['hasOptionalSkill'][i]['title']\n",
    "        except:\n",
    "            break\n",
    "        job = {\n",
    "        'job_title' : job_title,\n",
    "        'essential_skill': essential_skill,\n",
    "        'optional_skill' : optional_skill\n",
    "        }   \n",
    "        joblist.append(job)\n",
    "        jobs = pd.DataFrame(data=joblist, columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "    return jobs\n",
    "\n",
    "def UG(page, tag):\n",
    "    \"\"\"\n",
    "    This function find link to the relevant educations on Uddannelsesguiden.dk and append the links in a list\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    url = f\"https://www.ug.dk/search/{tag}?page={page}\"\n",
    "    r = requests.get(url, headers)\n",
    "    time.sleep(0.5)\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "    divs = soup.find_all(\"div\", class_=\"node node-uddannelse node-teaser clearfix\")\n",
    "    list_of_articles = []\n",
    "    for i in range(len(divs)):\n",
    "        list_of_articles.append(divs[i].find('a')['href'])\n",
    "    list_of_articles_final = []\n",
    "    for link in list_of_articles:\n",
    "        if '/kandidatuddannelser/' in link:\n",
    "            list_of_articles_final.append(link)\n",
    "\n",
    "    return list_of_articles_final\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function clean all text files and replaces the words\n",
    "    \"\"\"\n",
    "    # text.split(' ')\n",
    "    # text = text.replace('[','').replace(']', '')\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$|[^a-zA-ZæøåÆøå]+|(<li>)|(</li>)|</p>|<p>|\\xa0|<ul>|</ul>|'|-| +|,| ,|</a>|<a>|<a|[.]|[:]|\\n|[?]\", \" \", \n",
    "    text)\n",
    "    # text = re.sub(\"[^a-zA-Z0-9 -]\", '', text)\n",
    "    # text = re.sub(\"(<li>)|(</li>)|</p>|<p>|\\xa0|<ul>|</ul>|'|-| +|,| ,|</a>|<a>|<a|[.]|[:]|\\n|[?]\", '', text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text\n",
    "    \n",
    "def extract_UG(link):\n",
    "    \"\"\"\n",
    "    This function find relevant text on Uddannelsesguiden.dk and append the text in a list\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    url = 'http://ug.dk' + link\n",
    "    r = requests.get(url, headers)\n",
    "    time.sleep(0.5)\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "\n",
    "    divs_intro = soup.find_all(\"div\", class_=\"region region-content\")\n",
    "    for item in divs_intro:\n",
    "        intro = item.find_all(\"p\")[0:6]\n",
    "        indhold = item.find_all('ul')[0:1]\n",
    "        outro = item.find_all('p')[19:22]\n",
    "    \n",
    "    text = ' '.join([str(i) for i in intro]) + ' '.join([str(i) for i in indhold]) + ' '.join([str(i) for i in outro])\n",
    "    text = clean_text(text)\n",
    "    return text\n",
    "\n",
    "def write_text(list_name, list_):\n",
    "    \"\"\"\n",
    "    This function takes a list as an input and saves the file in a txt format\n",
    "    \"\"\"\n",
    "    with open(f\"{list_name}.txt\", \"w\", encoding='utf-8') as fp:\n",
    "        for item in list_:\n",
    "            fp.write(\"%s \" % item)\n",
    "    fp.close() \n",
    "\n",
    "def extract_ku(fag):\n",
    "    \"\"\"\n",
    "    This function scrapes relevant information from the wesite of university of copenhagen\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    url = f\"https://studier.ku.dk/kandidat/{fag}/faglig-profil-og-job/\"\n",
    "    r = requests.get(url, headers)\n",
    "    time.sleep(0.5)\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def transform_ku(soup):\n",
    "    \"\"\"\n",
    "    This function transforms relevant information from the wesite of university of copenhagen\n",
    "    \"\"\"\n",
    "    divs = soup.find_all(\"div\", class_=\"col-xs-12 col-sm-8 col-md-6 main-content\")\n",
    "    text_ku =[]\n",
    "    for i in range(len(divs)):\n",
    "        text_ku.append(divs[i].find_all(\"p\"))\n",
    "\n",
    "    divs2 = soup.find_all(\"div\", class_=\"col-xs-12 col-sm-8 col-md-6 main-content\")\n",
    "\n",
    "    for i in range(len(divs2)):\n",
    "        text_ku.append(divs2[i].find_all(\"ul\"))\n",
    "    \n",
    "    return text_ku\n",
    "\n",
    "def extract_au(fag):\n",
    "    \"\"\"\n",
    "    This function transforms relevant information from the wesite of university of aarhus\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    url = f\"https://bachelor.au.dk/{fag}\"\n",
    "    r = requests.get(url, headers)\n",
    "    time.sleep(0.5)\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def extract_aau(fag):\n",
    "    \"\"\"\n",
    "    This function transforms relevant information from the wesite of university of aalborg\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    url = f\"https://www.aau.dk/uddannelser/kandidat/{fag}\"\n",
    "    r = requests.get(url, headers)\n",
    "    time.sleep(0.5)\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def make_a_list(series):\n",
    "    \"\"\"\n",
    "    This function turn a series from af dataframe into a list, and cleans the data\n",
    "    \"\"\"\n",
    "    new_list = []\n",
    "    list1 = series.to_list()\n",
    "    for item in list1:\n",
    "        new_list.append(clean_text(item))\n",
    "    new_final_df = pd.DataFrame(data=new_list)\n",
    "\n",
    "    return new_final_df\n",
    "\n",
    "def generate_better_wordcloud(data, size):\n",
    "    \"\"\"\n",
    "    This function makes a wordcloud plot\n",
    "    \"\"\"\n",
    "    cloud = WordCloud(scale=3,\n",
    "                      max_words=None, #Maximum words in the WordCloud\n",
    "                      colormap='ocean', #Color of the WordCloud\n",
    "                      background_color='white',\n",
    "                      max_font_size=95,\n",
    "                      mask=None,\n",
    "                      relative_scaling=0.5,\n",
    "                      stopwords=stop_word, #Setting StopWords equal to the updated\n",
    "                      collocations=False).generate(data)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off') #No axis \n",
    "    plt.show()\n",
    "\n",
    "def frequency_plot(data):\n",
    "    \"\"\"\n",
    "    This function makes a frequency plot\n",
    "    \"\"\"\n",
    "    plot = FreqDist(data).most_common(20)\n",
    "    all_fdist = pd.Series(dict(plot))\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.barplot(x=all_fdist.values, y=all_fdist.index, ax=ax, color='navy', ci=None)\n",
    "    plt.ylabel('Words', fontsize=14)\n",
    "    plt.xlabel('Count', fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=16) #rotation=30\n",
    "    plt.show()\n",
    "\n",
    "def lemmatize_text(input):\n",
    "    \"\"\"\n",
    "    This function lemmatizes a string and appends the first apperance of the lemmatized string\n",
    "    \"\"\"\n",
    "    list_ = []\n",
    "    func = [lemmatizer.lemmatize('', i) for i in input]\n",
    "    for sublist in func:\n",
    "        list_.append(sublist[:1])\n",
    "\n",
    "    return list_\n",
    "\n",
    "def clean_jobindex(search_word):\n",
    "    \"\"\"\n",
    "    This function takes the links found in extract jobindex and finds all links in the jobindex.dk archive function.\n",
    "    Afterwards this function import all text from the first 4 pages of jobpostings and lemmatizes, cleans and removes stopwords.\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk', 'Name':'Nicolai Bernsen'}\n",
    "    final_job = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "    final_list = []\n",
    "    job_list = []\n",
    "\n",
    "    job = extract_jobindex(search_word, 4)\n",
    "    \n",
    "    for i in job:\n",
    "        url = f\"{i}\"\n",
    "        r = requests.get(url, headers)\n",
    "        time.sleep(0.5)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "        iframe = soup.find_all('iframe', class_='archive-content')\n",
    "        for i in iframe:\n",
    "            link = i['src']\n",
    "            info_job.append(link)\n",
    "\n",
    "    for item in info_job:\n",
    "        links = 'http://www.jobindexarkiv.dk/cgi/showarchive.cgi' + item\n",
    "        jobindex.append(links)\n",
    "\n",
    "    for i in jobindex:\n",
    "        url_ = f\"{i}\"\n",
    "        r = requests.get(url_, headers)\n",
    "        time.sleep(0.5)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        content = soup.find_all('body')\n",
    "        for i in content:\n",
    "            text = i.get_text()\n",
    "            flat_list.append(text)\n",
    "\n",
    "    job_ = ' '.join(str(i) for i in flat_list)\n",
    "    _job = nltk.word_tokenize(clean_text(job_))\n",
    "    job__ = lemmatize_text(_job)\n",
    "    for sublist in job__:\n",
    "        job_list.append(' '.join(sublist))\n",
    "\n",
    "    for word in job_list:\n",
    "        if word not in stop_word:\n",
    "            final_job.append(word)\n",
    "\n",
    "    return final_job\n",
    "\n",
    "def find_search_words(df, type_):\n",
    "    \"\"\"\n",
    "    This function find similarities between ESCO skills and boss-words in the Jobindex and Education lists\n",
    "    \"\"\"\n",
    "    search_words = []\n",
    "    job_list = []\n",
    "    final_list = []\n",
    "    searchword = df.loc[jobs['job_title'] == f'{type_}']\n",
    "    for value in searchword['optional_skill']:\n",
    "        search_words.append(value)\n",
    "    for value in searchword['essential_skill']:\n",
    "        search_words.append(value)\n",
    "    res = ' '.join(search_words)\n",
    "    res = nltk.word_tokenize(clean_text(res))\n",
    "    res_ = lemmatize_text(res)\n",
    "    for sublist in res_:\n",
    "        job_list.append(' '.join(sublist))\n",
    "\n",
    "    for word in job_list:\n",
    "        if word not in stop_word:\n",
    "            final_list.append(word)\n",
    "\n",
    "    result = []\n",
    "    for i in final_list:\n",
    "        [result.append(i) for i in final_list if i not in result]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping and cleaning Jobindex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobindex_psych = clean_jobindex('cand.psych')\n",
    "jobindex_oecon = clean_jobindex('cand.oecon')\n",
    "jobindex_pol = clean_jobindex('cand.scient.pol')\n",
    "jobindex_anth = clean_jobindex('cand.scient.anth')\n",
    "jobindex_soc = clean_jobindex('cand.scient.soc')\n",
    "\n",
    "jobindex_psych_full = ' '.join(jobindex_psych)\n",
    "jobindex_oecon_full = ' '.join(jobindex_oecon)\n",
    "jobindex_pol_full = ' '.join(jobindex_pol)\n",
    "jobindex_anth_full = ' '.join(jobindex_anth)\n",
    "jobindex_soc_full = ' '.join(jobindex_soc)\n",
    "\n",
    "print(f'Number of word in \\n psych: {len(jobindex_psych_full)}, \\n oecon: {len(jobindex_oecon_full)},\\n \\\n",
    "pol: {len(jobindex_pol_full)}, \\n anth: {len(jobindex_anth_full)}, \\n soc: {len(jobindex_soc_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Jobindex data using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(jobindex_oecon_full, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(jobindex_psych_full, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(jobindex_pol_full, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(jobindex_anth_full, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(jobindex_soc_full, (10,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'økonom':'cand.oecon', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esco_oecon = find_search_words(jobs, 'cand.oecon')\n",
    "esco_psych = find_search_words(jobs, 'cand.psych')\n",
    "esco_pol = find_search_words(jobs, 'cand.scient.pol')\n",
    "esco_anth = find_search_words(jobs, 'cand.scient.anth')\n",
    "esco_soc = find_search_words(jobs, 'cand.scient.soc')\n",
    "\n",
    "print(f'Number of skills in: \\n psych: {len(esco_psych)}, \\n oecon: {len(esco_oecon)}, \\\n",
    "    \\n pol: {len(esco_pol)}, \\n anth: {len(esco_anth)}, \\n soc: {len(esco_soc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding matches between Jobindex data and Esco skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches(esco, jobindex):\n",
    "    matches = [x for x in esco if x in jobindex]\n",
    "    return matches\n",
    "\n",
    "print('Found the following matches between Esco Skills and skills in Jobindex Data:')\n",
    "print(f'cand.oecon = {matches(esco_oecon, jobindex_oecon_full)} \\n \\\n",
    "cand.psych = {matches(esco_psych, jobindex_psych_full)} \\n \\\n",
    "cand.scient.soc = {matches(esco_soc, jobindex_soc_full)} \\n \\\n",
    "cand.scient.ant = {matches(esco_anth, jobindex_anth_full)} \\n \\\n",
    "cand.scient.pol = {matches(esco_pol, jobindex_pol_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping KU and other educational institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ku_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'statskundskab', 'antropologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    soup = extract_ku(k)\n",
    "    ku_list.append(transform_ku(soup))\n",
    "\n",
    "ku_df = pd.DataFrame(data=ku_list)\n",
    "\n",
    "#Merging the two colums \n",
    "ku_df=ku_df[0]+ku_df[1]\n",
    "\n",
    "#Making the object a dataframe\n",
    "ku_df=pd.DataFrame(ku_df)\n",
    "\n",
    "ku_df=pd.DataFrame.transpose(ku_df)\n",
    "\n",
    "ku_df.columns=['cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', 'cand.oecon_ku']\n",
    "soup = extract_au('statskundskab')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_stats = text[75:77]\n",
    "\n",
    "stats_df = pd.DataFrame(data=text_stats, columns=['cand.scient.pol_au'])\n",
    "stats = pd.DataFrame([', '.join(stats_df['cand.scient.pol_au'].to_list())], columns=['cand.scient.pol_au'])\n",
    "soup = extract_au('oekonomi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_oek = text[60:62]\n",
    "\n",
    "oek_df = pd.DataFrame(data=text_oek, columns=['cand.oecon_au'])\n",
    "oek = pd.DataFrame([', '.join(oek_df['cand.oecon_au'].to_list())], columns=['cand.oecon_au'])\n",
    "soup = extract_au('antropologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_ant = text[50:55]\n",
    "\n",
    "ant_df = pd.DataFrame(data=text_ant, columns=['cand.scient.anth_au'])\n",
    "ant = pd.DataFrame([', '.join(ant_df['cand.scient.anth_au'].to_list())], columns=['cand.scient.anth_au'])\n",
    "soup = extract_au('psykologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_psyk = text[74:78]\n",
    "\n",
    "psyk_df = pd.DataFrame(data=text_psyk, columns=['cand.psych_au'])\n",
    "psyk = pd.DataFrame([', '.join(psyk_df['cand.psych_au'].to_list())], columns=['cand.psych_au'])\n",
    "frames = [ant, stats, psyk, oek]\n",
    "\n",
    "au_df = pd.concat(frames, axis=1)\n",
    "\n",
    "au_df_list = au_df['cand.scient.anth_au'].to_list()\n",
    "au_list = []\n",
    "\n",
    "for i in au_df_list:\n",
    "    au_list.append(clean_text(i))\n",
    "\n",
    "au_list\n",
    "def transform_aau(soup):\n",
    "\n",
    "    divs = soup.find_all(\"main\", class_=\"Main_Main__2KIvG\")\n",
    "\n",
    "    for item in divs:\n",
    "        text_aau = item.find_all()[0].text.strip()\n",
    "\n",
    "        aau_text = {\n",
    "            \"text_aau\" : text_aau, \n",
    "        }\n",
    "        aau_list.append(aau_text)\n",
    "\n",
    "        text_aau = clean_text(text_aau)\n",
    "        \n",
    "    return text_aau\n",
    "\n",
    "aau_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    try: \n",
    "        soup = extract_aau(k)\n",
    "        transform_aau(soup)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "aau_df = pd.DataFrame(data=aau_list)\n",
    "\n",
    "aau_df=pd.DataFrame.transpose(aau_df)\n",
    "\n",
    "aau_df.columns=['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau']\n",
    "\n",
    "aau_df = aau_df.reset_index(drop=True)\n",
    "\n",
    "aau_df\n",
    "\n",
    "## Combining Dataframes\n",
    "merge_frames = [ku_df, au_df, aau_df]\n",
    "\n",
    "combined_df = pd.concat(merge_frames, axis=1)\n",
    "\n",
    "combined_df['cand.psych_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.psych_ku']]\n",
    "combined_df['cand.scient.pol_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.pol_ku']]\n",
    "combined_df['cand.oecon_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.oecon_ku']]\n",
    "combined_df['cand.scient.anth_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.anth_ku']]\n",
    "combined_df['cand.scient.soc_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.soc_ku']]\n",
    "\n",
    "combined_df['cand.psych'] = combined_df['cand.psych_aau'] + combined_df['cand.psych_au'] + combined_df['cand.psych_ku_string']\n",
    "combined_df['cand.scient.anth'] = combined_df['cand.scient.anth_au'] + combined_df['cand.scient.anth_ku_string']\n",
    "combined_df['cand.scient.pol'] = combined_df['cand.scient.pol_au'] + combined_df['cand.scient.pol_ku_string']\n",
    "combined_df['cand.scient.soc'] = combined_df['cand.scient.soc_aau'] + combined_df['cand.scient.soc_ku_string']\n",
    "combined_df['cand.oecon'] = combined_df['cand.oecon_aau'] + combined_df['cand.oecon_au'] + combined_df['cand.oecon_ku_string']\n",
    "combined_df.drop(['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau', 'cand.psych_au', \\\n",
    "                  'cand.scient.anth_au', 'cand.scient.pol_au','cand.oecon_au', \\\n",
    "                  'cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', \\\n",
    "                  'cand.oecon_ku', 'cand.psych_ku_string', 'cand.scient.pol_ku_string', 'cand.oecon_ku_string', \\\n",
    "                  'cand.scient.anth_ku_string', 'cand.scient.soc_ku_string'], axis=1, inplace=True)\n",
    "university_df = combined_df\n",
    "university_df\n",
    "\n",
    "## Scraping UG\n",
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]\n",
    "psych = clean_text(extract_UG(education_url[0][0]))\n",
    "oecon = extract_UG(education_url[1][0])\n",
    "pol = extract_UG(education_url[2][0])\n",
    "anth = extract_UG(education_url[3][0])\n",
    "soc = extract_UG(education_url[4][0])\n",
    "\n",
    "## Combining skills from UG and Universities\n",
    "strings_psych = psych + university_df['cand.psych']\n",
    "strings_oecon = oecon + university_df['cand.oecon']\n",
    "strings_pol = pol + university_df['cand.scient.pol']\n",
    "strings_anth = anth + university_df['cand.scient.anth']\n",
    "strings_soc = soc + university_df['cand.scient.soc']\n",
    "\n",
    "\n",
    "psych_comb = \" \".join(strings_psych)\n",
    "oecon_comb = \" \".join(strings_oecon)\n",
    "pol_comb = \" \".join(strings_pol)\n",
    "anth_comb = \" \".join(strings_anth)\n",
    "soc_comb = \" \".join(strings_soc)\n",
    "psych_series = pd.Series(psych_comb)\n",
    "oecon_series = pd.Series(oecon_comb)\n",
    "pol_series = pd.Series(pol_comb)\n",
    "anth_series = pd.Series(anth_comb)\n",
    "soc_series = pd.Series(soc_comb)\n",
    "\n",
    "psych_df = pd.DataFrame(psych_series)\n",
    "oecon_df = pd.DataFrame(oecon_series)\n",
    "pol_df = pd.DataFrame(pol_series)\n",
    "anth_df = pd.DataFrame(anth_series)\n",
    "soc_df = pd.DataFrame(soc_series)\n",
    "\n",
    "psych_df['cand.psych'] = psych_df[0]\n",
    "oecon_df['cand.oecon'] = oecon_df[0]\n",
    "pol_df['cand.scient.pol'] = pol_df[0]\n",
    "anth_df['cand.scient.anth'] = anth_df[0]\n",
    "soc_df['cand.scient.soc'] = soc_df[0]\n",
    "final_df = pd.concat([psych_df['cand.psych'], oecon_df['cand.oecon'], pol_df['cand.scient.pol'], \\\n",
    "                      anth_df['cand.scient.anth'], soc_df['cand.scient.soc']], axis=1)\n",
    "\n",
    "final_df = make_a_list(final_df['cand.psych']).append(make_a_list(final_df['cand.oecon'])).append(make_a_list(final_df['cand.scient.pol']))\\\n",
    "    .append(make_a_list(final_df['cand.scient.anth'])).append(make_a_list(final_df['cand.scient.soc']))\n",
    "\n",
    "final_df = final_df.T\n",
    "final_df.columns = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(nltk.word_tokenize)\n",
    "final_df['cand.oecon'] = final_df['cand.oecon'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(nltk.word_tokenize)\n",
    "\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.oecon'] = final_df['cand.oecon'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(lambda words: [word for word in words if word not in stop_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(series):\n",
    "    \"\"\"\n",
    "    This function lemmatizes and cleans data from dataframes and turn them into lists.\n",
    "    \"\"\"\n",
    "    final_list_ = []\n",
    "    job_list = []\n",
    "    for i in series:\n",
    "        list_ = list(i)\n",
    "    list_ = ' '.join(list_)\n",
    "    final_list = list_.split(' ')\n",
    "    job = lemmatize_text(final_list)\n",
    "    for sublist in job:\n",
    "        job_list.append(' '.join(sublist))\n",
    "\n",
    "    for word in job_list:\n",
    "        if word not in stop_word:\n",
    "            final_list_.append(word)\n",
    "\n",
    "    return final_list_\n",
    "\n",
    "udd_oecon_list = df_to_list(final_df['cand.oecon'])\n",
    "udd_psych_list = df_to_list(final_df['cand.psych'])\n",
    "udd_pol_list = df_to_list(final_df['cand.scient.pol'])\n",
    "udd_anth_list = df_to_list(final_df['cand.scient.anth'])\n",
    "udd_soc_list = df_to_list(final_df['cand.scient.soc'])\n",
    "\n",
    "udd_oecon_string = ' '.join(udd_oecon_list)\n",
    "udd_psych_string = ' '.join(udd_psych_list)\n",
    "udd_anth_string = ' '.join(udd_anth_list)\n",
    "udd_pol_string = ' '.join(udd_pol_list)\n",
    "udd_soc_string = ' '.join(udd_soc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of Wordcloud for Educations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(udd_oecon_string, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(udd_psych_string, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(udd_anth_string, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(udd_soc_string, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_better_wordcloud(udd_pol_string, (10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(words_to_search, list_, name):\n",
    "    for i in words_to_search:\n",
    "        if i in list_:\n",
    "            print(f'{i} is present in the list of {name}')\n",
    "\n",
    "find_words(esco_oecon, udd_oecon_list, 'oecon')\n",
    "find_words(esco_anth, udd_anth_list, 'anth')\n",
    "find_words(esco_pol, udd_pol_list, 'pol')\n",
    "find_words(esco_psych, udd_psych_list, 'psych')\n",
    "find_words(esco_soc, udd_soc_list, 'soc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the UFM data source https://datavarehus.ufm.dk/rapporter/ledighed does not provide any API we download the data\n",
    "# and store it in our github repo:\n",
    "df_ufm_1 = pd.read_csv(\"ledighed_drivkraft.csv\", sep=';', decimal=',', header=[1], skipinitialspace=True)\n",
    "\n",
    "df_ufm_2 = pd.read_csv(\"ledighed.csv\", skipinitialspace = True, sep=',', decimal='.')\n",
    "\n",
    "# We make a sub dataframe for the educations within the fields of social sciences (samf):\n",
    "df_ss = df_ufm_2.iloc[118:132, :].copy()\n",
    "df_ss.reset_index(drop=True)\n",
    "\n",
    "# Replace \",\" notation with \".\" notation and replace \"%\" signs with an empty string:\n",
    "df_ss['Ledighedsgrad'] = [x.replace(',', '.') for x in df_ss['Ledighedsgrad']].copy()\n",
    "df_ss['Ledighedsgrad'] = [x.replace('%', '') for x in df_ss['Ledighedsgrad']].copy()\n",
    "\n",
    "# Convert colum 'Ledighedsgrad' into float type:\n",
    "df_ss['Ledighedsgrad'] = df_ss['Ledighedsgrad'].astype(float)\n",
    "\n",
    "ledighed_jura = df_ss.iloc[0:3, 4].sum() / 3\n",
    "gen_ledighed_jura = \"{:.2f}\".format(ledighed_jura)\n",
    "print(f'Den gennemsnitlige ledighed for jura er {gen_ledighed_jura} pct.')\n",
    "\n",
    "ledighed_ervøko = df_ss.iloc[3:6, 4].sum() / 3\n",
    "gen_ledighed_ervøko = \"{:.2f}\".format(ledighed_ervøko)\n",
    "print(f'Den gennemsnitlige ledighed for erhvervsøkonomi er {gen_ledighed_ervøko} pct.')\n",
    "\n",
    "ledighed_forval = df_ss.iloc[6:9, 4].sum() / 3\n",
    "gen_ledighed_forval = \"{:.2f}\".format(ledighed_forval)\n",
    "print(f'Den gennemsnitlige ledighed for forvaltning er {gen_ledighed_forval} pct.')\n",
    "\n",
    "ledighed_psyko = df_ss.iloc[9:12, 4].sum() / 3\n",
    "gen_ledighed_psyko = \"{:.2f}\".format(ledighed_psyko)\n",
    "print(f'Den gennemsnitlige ledighed for psykologi er {gen_ledighed_psyko} pct.')\n",
    "\n",
    "ledighed_øvrig = df_ss.iloc[12:15, 4].sum() / 3\n",
    "gen_ledighed_øvrig = \"{:.2f}\".format(ledighed_øvrig)\n",
    "print(f'Den gennemsnitlige ledighed for øvrige samfundsvidenskabelige uddannelser er {gen_ledighed_øvrig} pct.')\n",
    "\n",
    "colors = ['#069AF3','lightblue','#929591','darkseagreen','teal']\n",
    "objects = ('Law', 'Business Economics', 'Administration', 'Psychology', 'Others')\n",
    "y_pos = np.arange(len(objects))\n",
    "values = [ledighed_jura, ledighed_ervøko, ledighed_forval, ledighed_psyko, ledighed_øvrig]\n",
    "plt.rcParams['axes.facecolor'] = 'white' # change background color\n",
    "plt.bar(y_pos, values, align='center', alpha=0.7, color=colors)\n",
    "plt.xticks(y_pos, objects, rotation=0, fontsize=8)\n",
    "plt.ylabel('Unemployment Rate, %', fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.savefig('Unemployment_ss2.png', facecolor=\"white\", bbox_inches='tight',transparent=True, pad_inches=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ufm_1.drop(df_ufm_1.tail(3).index,inplace=True) # drop last three rows\n",
    "df_ufm_1 = df_ufm_1.replace(' %', '', regex=True)\n",
    "df_ufm_1 = df_ufm_1.replace(',', '.', regex=True)\n",
    "df_ufm_1['Ledighedsgrad'] = [x.replace(',', '.') for x in df_ufm_1['Ledighedsgrad']].copy()\n",
    "df_ufm_1['Ledighedsgrad'] = [x.replace('%', '') for x in df_ufm_1['Ledighedsgrad']].copy()\n",
    "\n",
    "df_ufm_1['Ledighedsgrad.1'] = [x.replace(',', '.') for x in df_ufm_1['Ledighedsgrad.1']].copy()\n",
    "df_ufm_1['Ledighedsgrad.1'] = [x.replace('%', '') for x in df_ufm_1['Ledighedsgrad.1']].copy()\n",
    "\n",
    "df_ufm_1['Ledighedsgrad.2'] = [x.replace(',', '.') for x in df_ufm_1['Ledighedsgrad.2']].copy()\n",
    "df_ufm_1['Ledighedsgrad.2'] = [x.replace('%', '') for x in df_ufm_1['Ledighedsgrad.2']].copy()\n",
    "\n",
    "df_ufm_1['Ledighedsgrad'] = df_ufm_1['Ledighedsgrad']\n",
    "\n",
    "df_ufm_1['Ledighedsgrad.1'] = df_ufm_1['Ledighedsgrad.1']\n",
    "\n",
    "df_ufm_1['Ledighedsgrad.2'] = df_ufm_1['Ledighedsgrad.2']\n",
    "\n",
    "antal_ledige_samf_2017 = round((float(df_ufm_1.iloc[3, 2]) / 100) * (float(df_ufm_1.iloc[3, 3]) * 1000))\n",
    "\n",
    "antal_ledige_samf_2018 = round((float(df_ufm_1.iloc[3, 4]) / 100) * (float(df_ufm_1.iloc[3, 5]) * 1000))\n",
    "\n",
    "antal_ledige_samf_2019 = round((float(df_ufm_1.iloc[3, 6]) / 100) * (float(df_ufm_1.iloc[3, 7]) * 1000))\n",
    "\n",
    "antal_ledige_samf_samlet = antal_ledige_samf_2017 + antal_ledige_samf_2018 + antal_ledige_samf_2019\n",
    "antal_ledige_human_2017 = round((float(df_ufm_1.iloc[4, 2]) / 100) * (float(df_ufm_1.iloc[4, 3]) * 1000))\n",
    "\n",
    "antal_ledige_human_2018 = round((float(df_ufm_1.iloc[4, 4]) / 100) * (float(df_ufm_1.iloc[4, 5]) * 1000))\n",
    "\n",
    "antal_ledige_human_2019 = round((float(df_ufm_1.iloc[4, 6]) / 100) * (float(df_ufm_1.iloc[4, 7]) * 1000))\n",
    "\n",
    "antal_ledige_human_samlet = antal_ledige_human_2017 + antal_ledige_human_2018 + antal_ledige_human_2019\n",
    "antal_ledige_teknik_2017 = round((float(df_ufm_1.iloc[5, 2]) / 100) * (float(df_ufm_1.iloc[5, 3]) * 1000))\n",
    "\n",
    "antal_ledige_teknik_2018 = round((float(df_ufm_1.iloc[5, 4]) / 100) * (float(df_ufm_1.iloc[5, 5]) * 1000))\n",
    "\n",
    "antal_ledige_teknik_2019 = round((float(df_ufm_1.iloc[5, 6]) / 100) * (float(df_ufm_1.iloc[5, 7]) * 1000))\n",
    "\n",
    "antal_ledige_teknik_samlet = antal_ledige_teknik_2017 + antal_ledige_teknik_2018 + antal_ledige_teknik_2019\n",
    "antal_ledige_natur_2017 = round((float(df_ufm_1.iloc[6, 2]) / 100) * (float(df_ufm_1.iloc[6, 3]) * 1000))\n",
    "\n",
    "antal_ledige_natur_2018 = round((float(df_ufm_1.iloc[6, 4]) / 100) * (float(df_ufm_1.iloc[6, 5]) * 1000))\n",
    "\n",
    "antal_ledige_natur_2019 = round((float(df_ufm_1.iloc[6, 6]) / 100) * (float(df_ufm_1.iloc[6, 7]) * 1000))\n",
    "\n",
    "antal_ledige_natur_samlet = antal_ledige_natur_2017 + antal_ledige_natur_2018 + antal_ledige_natur_2019\n",
    "print(antal_ledige_natur_samlet)\n",
    "antal_ledige_sundh_2017 = round((float(df_ufm_1.iloc[7, 2]) / 100) * (float(df_ufm_1.iloc[7, 3]) * 1000))\n",
    "\n",
    "antal_ledige_sundh_2018 = round((float(df_ufm_1.iloc[7, 4]) / 100) * (float(df_ufm_1.iloc[7, 5]) * 1000))\n",
    "\n",
    "antal_ledige_sundh_2019 = round((float(df_ufm_1.iloc[7, 6]) / 100) * (float(df_ufm_1.iloc[7, 7]) * 1000))\n",
    "\n",
    "antal_ledige_sundh_samlet = antal_ledige_sundh_2017 + antal_ledige_sundh_2018 + antal_ledige_sundh_2019\n",
    "print(antal_ledige_sundh_samlet)\n",
    "# Pie chart\n",
    "labels = [\"Social Sciences\", \"Humanities\", \"Technology\", \"Natural Sciences\", \"Health Sciences\"]\n",
    "\n",
    "#colors\n",
    "colors = ['#069AF3','lightblue','#929591','darkseagreen','teal']\n",
    "y = np.array([antal_ledige_samf_samlet, antal_ledige_human_samlet, antal_ledige_teknik_samlet, antal_ledige_natur_samlet, antal_ledige_sundh_samlet])\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "patches, texts, autotexts = ax1.pie(y, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, textprops={'fontsize': 18})\n",
    "for text in texts:\n",
    "    text.set_color('black')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('black')\n",
    "plt.rcParams[\"figure.figsize\"] = (16,10)\n",
    "#draw circle\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle\n",
    "ax1.axis('equal')\n",
    "plt.savefig('Unemployment_driving_force.png', facecolor=\"white\", bbox_inches='tight',transparent=True, pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initally we define the values of the variables \n",
    "unem_psy = 10\n",
    "unem_soc = 15\n",
    "unem_ant = 20\n",
    "unem_pol = 5\n",
    "unem_eco = 2\n",
    "cosine_psy = 0.280990\n",
    "cosine_soc = 0.163015\n",
    "cosine_ant = 0.081864\n",
    "cosine_pol = 0.140346\n",
    "cosine_eco = 0.424609\n",
    "\n",
    "# We contruct a numpy array to fit the relevant data into a pandas DF  \n",
    "\n",
    "array = np.array([[cosine_ant, unem_ant],[cosine_soc, unem_soc],[cosine_psy, unem_psy],[cosine_pol, unem_pol],[cosine_eco, unem_eco]])\n",
    "\n",
    "# Creating a list of column and index names\n",
    "index_values = ['Anthropology', 'Sociology', 'Psychology',\n",
    "                'Political Science', 'Economics'] \n",
    "\n",
    "column_values = ['Cosine Similarity', 'Unemployment_Diff']\n",
    " \n",
    "# Creating the dataframe\n",
    "df = pd.DataFrame(data = array,\n",
    "                  index = index_values,\n",
    "                  columns = column_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define response variable\n",
    "y = df['Unemployment_Diff']\n",
    "\n",
    "# Define predictor variables\n",
    "x = df['Cosine Similarity']\n",
    "\n",
    "# Add constant to predictor variables\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(10,5)})\n",
    "\n",
    "# View model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Make scatter plot\n",
    "figure, axes = plt.subplots(1, 2, sharex=True)\n",
    "axes[0].set_title('(I) Regression plot excluded outlier', fontweight=\"bold\")\n",
    "axes[1].set_title('(II) Regression plot included outlier', fontweight=\"bold\")\n",
    "sns.scatterplot(ax=axes[1], x='Cosine Similarity', # Horizontal axis\n",
    "                y='Unemployment_Diff', # Vertical axis\n",
    "                data=df) # Data source\n",
    "\n",
    "# Change color of outlier data point on the plot\n",
    "plt.scatter(df[\"Cosine Similarity\"].iloc[-2], df.Unemployment_Diff.iloc[-2], color='orange')\n",
    "\n",
    "# Reshaping the independent and dependent variable for regression\n",
    "y1 = df[\"Unemployment_Diff\"].values.reshape(-1,1)\n",
    "X1 = df[\"Cosine Similarity\"].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "# Setting Simple Linear Regression up\n",
    "simple1 = LinearRegression();\n",
    "\n",
    "# Fitting to the model\n",
    "simple1.fit(X1,y1);\n",
    "\n",
    "# Calculating prediction\n",
    "pred1 = simple1.predict(X1)\n",
    "\n",
    "# Dropping row with political science from DataFrame\n",
    "df.drop([\"Political Science\"], axis=0, inplace=True)\n",
    "\n",
    "# Redefining the independent and dependent variable\n",
    "y2 = df[\"Unemployment_Diff\"].values.reshape(-1,1)\n",
    "X2 = df[\"Cosine Similarity\"].values.reshape(-1,1)\n",
    "\n",
    "# Setting Simple Linear Regression up again\n",
    "simple2 = LinearRegression();\n",
    "\n",
    "# Fitting to the model\n",
    "simple2.fit(X2,y2);\n",
    "\n",
    "# Calculating new prediction\n",
    "pred2 = simple2.predict(X2)\n",
    "\n",
    "\n",
    "sns.scatterplot(ax=axes[0], x='Cosine Similarity', # Horizontal axis\n",
    "                y='Unemployment_Diff', # Vertical axis\n",
    "                data=df) # Data source\n",
    "axes[1].plot(X1,pred1, '-b')\n",
    "axes[0].plot(X2,pred2, '-r')\n",
    "# Set x-axis labels\n",
    "axes[0].set_xlabel(\"tf-idf Cosine Similarity\")\n",
    "axes[1].set_xlabel(\"tf-idf Cosine Similarity\")\n",
    "# Set y-axis label\n",
    "axes[0].set_ylabel(\"Core Unemployment, per cent\")\n",
    "axes[1].set_ylabel(\"Core Unemployment, per cent\")\n",
    "\n",
    "# label points on the plot (I)\n",
    "axes[0].text(0.086, 19.9, \"cand.psych\", horizontalalignment='left', size='small', color='black')\n",
    "axes[0].text(0.159, 15, \"cand.scient.soc\", horizontalalignment='right', size='small', color='black')\n",
    "axes[0].text(0.285 , 10, \"cand.scient.anth\", horizontalalignment='left', size='small', color='black')\n",
    "axes[0].text(0.415 , 2.1, \"cand.econ\", horizontalalignment='right', size='small', color='black')\n",
    "# label points on the plot (II)\n",
    "axes[1].text(0.15, 5, \"cand.scient.pol\", horizontalalignment='left', size='small', color='black')\n",
    "axes[1].text(0.086, 19.9, \"cand.psych\", horizontalalignment='left', size='small', color='black')\n",
    "axes[1].text(0.17, 15, \"cand.scient.soc\", horizontalalignment='left', size='small', color='black')\n",
    "axes[1].text(0.287 , 10, \"cand.scient.anth\", horizontalalignment='left', size='small', color='black')\n",
    "axes[1].text(0.415 , 2.1, \"cand.econ\", horizontalalignment='right', size='small', color='black')\n",
    "# plt.savefig('Regression_fig.png', bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Redefine response variable after the outlier has been dropped\n",
    "y = df['Unemployment_Diff']\n",
    "\n",
    "# Redefine predictor variables\n",
    "x = df['Cosine Similarity']\n",
    "\n",
    "# Add constant to predictor variables\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# Fit linear regression model\n",
    "model = sm.OLS(y, x).fit()\n",
    "\n",
    "# view model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectorizer(input_udd, input_jobindex):\n",
    "    \"\"\"\n",
    "    This funtion defines a vectorizer to be used on string data from jobindex.dk and education websites.\n",
    "    It then calculates the cosine similarity between the strings.\n",
    "    \"\"\"\n",
    "    list_ = []\n",
    "    _list = []\n",
    "    vector = TfidfVectorizer(lowercase=False)\n",
    "    analyzer = ([input_udd] + [input_jobindex]) #The order does not make a difference here\n",
    "    ret = vector.fit_transform(analyzer)\n",
    "    pairwise_similarity = ret * ret.T\n",
    "    array = pairwise_similarity.toarray()\n",
    "    for cell in np.nditer(array):\n",
    "        if cell < 0.9 and cell > 0.001:\n",
    "            list_.append(cell)\n",
    "\n",
    "    list_ = list_[1].tolist()\n",
    "\n",
    "    return list_\n",
    "\n",
    "list_ = (make_vectorizer(udd_oecon_string, jobindex_oecon_full),\n",
    "make_vectorizer(udd_psych_string, jobindex_psych_full), \n",
    "make_vectorizer(udd_pol_string, jobindex_pol_full), \n",
    "make_vectorizer(udd_soc_string, jobindex_soc_full),\n",
    "make_vectorizer(udd_anth_string, jobindex_anth_full))\n",
    "\n",
    "df_sim = pd.DataFrame(list_)\n",
    "df_tfidf = df_sim.rename(index={0:'cand.oecon', 1:'cand.psych', 2:'cand.scient.pol', 3:'cand.scient.soc', 4:'cand.scient.anth'})\n",
    "df_tfidf.columns=['Tfidf: Cosine Similarity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similarities with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecon_udd_nlp = nlp(udd_oecon_string)\n",
    "soc_udd_nlp = nlp(udd_soc_string)\n",
    "anth_udd_nlp = nlp(udd_anth_string)\n",
    "psych_udd_nlp = nlp(udd_psych_string)\n",
    "pol_udd_nlp = nlp(udd_pol_string)\n",
    "\n",
    "oecon_job_nlp = nlp(jobindex_oecon_full)\n",
    "soc_job_nlp = nlp(jobindex_soc_full)\n",
    "anth_job_nlp = nlp(jobindex_anth_full)\n",
    "psych_job_nlp = nlp(jobindex_psych_full)\n",
    "pol_job_nlp = nlp(jobindex_pol_full)\n",
    "\n",
    "print(f'cand.oecon similarity: {oecon_job_nlp.similarity(oecon_udd_nlp)} \\n \\\n",
    "cand.soc similarity: {soc_job_nlp.similarity(soc_udd_nlp)} \\n \\\n",
    "cand.anth similarity: {anth_job_nlp.similarity(anth_udd_nlp)} \\n \\\n",
    "cand.psych similarity: {psych_job_nlp.similarity(psych_udd_nlp)} \\n \\\n",
    "cand.pol similarity: {pol_job_nlp.similarity(pol_udd_nlp)}')\n",
    "\n",
    "list_spacy = (oecon_job_nlp.similarity(oecon_udd_nlp), soc_job_nlp.similarity(soc_udd_nlp), anth_job_nlp.similarity(anth_udd_nlp), \n",
    "psych_job_nlp.similarity(psych_udd_nlp), pol_job_nlp.similarity(pol_udd_nlp))\n",
    "df_spacy = pd.DataFrame(data=list_spacy, columns=['SpaCy: Cosine Similarity'])\n",
    "df_spacy = df_spacy.rename(index={0:'cand.oecon', 1:'cand.psych', 2:'cand.scient.pol', 3:'cand.scient.soc', 4:'cand.scient.anth'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jobindex_without_stopwords(search_word):\n",
    "    \"\"\"\n",
    "    This function does the same as clean_jobindex just without lemmatizing or removing stopwords. \n",
    "    This way string data is being prepared to be used in the BERT model.\n",
    "    \"\"\"\n",
    "    final_job = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "    final_list = []\n",
    "    job_list = []\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "    job = extract_jobindex(search_word, 4)\n",
    "    \n",
    "    for i in job:\n",
    "        url = f\"{i}\"\n",
    "        r = requests.get(url, headers)\n",
    "        time.sleep(0.5)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "        iframe = soup.find_all('iframe', class_='archive-content')\n",
    "        for i in iframe:\n",
    "            link = i['src']\n",
    "            info_job.append(link)\n",
    "\n",
    "    for item in info_job:\n",
    "        links = 'http://www.jobindexarkiv.dk/cgi/showarchive.cgi' + item\n",
    "        jobindex.append(links)\n",
    "\n",
    "    for i in jobindex:\n",
    "        url_ = f\"{i}\"\n",
    "        r = requests.get(url_, headers)\n",
    "        time.sleep(0.5)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        content = soup.find_all('body')\n",
    "        for i in content:\n",
    "            text = i.get_text()\n",
    "            flat_list.append(text)\n",
    "    job_ = ' '.join(str(i) for i in flat_list)\n",
    "    _job = clean_text(job_)\n",
    "    \n",
    "    return job_list\n",
    "\n",
    "def udd_without_stopwords(series):\n",
    "    final_list_ = []\n",
    "    job_list = []\n",
    "    for i in series:\n",
    "        list_ = list(i)\n",
    "    list_ = ' '.join(list_)\n",
    "\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udd_oecon_bert = udd_without_stopwords(final_df['cand.oecon'])\n",
    "udd_psych_bert = udd_without_stopwords(final_df['cand.psych'])\n",
    "udd_pol_bert = udd_without_stopwords(final_df['cand.scient.pol'])\n",
    "udd_anth_bert = udd_without_stopwords(final_df['cand.scient.anth'])\n",
    "udd_soc_bert = udd_without_stopwords(final_df['cand.scient.soc'])\n",
    "\n",
    "jobindex_bert_psych = jobindex_without_stopwords('cand.psych')\n",
    "jobindex_bert_oecon = jobindex_without_stopwords('cand.oecon')\n",
    "jobindex_bert_pol = jobindex_without_stopwords('cand.scient.pol')\n",
    "jobindex_bert_anth = jobindex_without_stopwords('cand.scient.anth')\n",
    "jobindex_bert_soc = jobindex_without_stopwords('cand.scient.soc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "def bert_model_simmilarities(udd_list, jobindex_list):\n",
    "    list_ = []\n",
    "    document_embeddings = sbert_model.encode([udd_list, jobindex_list])\n",
    "    similarities = cosine_similarity(document_embeddings)\n",
    "    for cell in np.nditer(similarities):\n",
    "        if cell < 0.9 and cell > 0.1:\n",
    "            list_.append(cell)\n",
    "\n",
    "    return list_[1].tolist()\n",
    "\n",
    "arr_oecon = bert_model_simmilarities(udd_oecon_list, jobindex_oecon)\n",
    "arr_psych = bert_model_simmilarities(udd_psych_list, jobindex_psych)\n",
    "arr_pol = bert_model_simmilarities(udd_pol_list, jobindex_pol)\n",
    "arr_soc = bert_model_simmilarities(udd_soc_list, jobindex_soc)\n",
    "arr_anth = bert_model_simmilarities(udd_anth_list, jobindex_anth)\n",
    "\n",
    "list_ = (arr_oecon, arr_psych, arr_pol, arr_soc, arr_anth)\n",
    "df_bert = pd.DataFrame(list_)\n",
    "\n",
    "df_bert = df_bert.rename(index={0:'cand.oecon', 1:'cand.psych', 2:'cand.scient.pol', 3:'cand.scient.soc', 4:'cand.scient.anth'})\n",
    "df_bert.columns=['Bert: Cosine Similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_oecon_2 = bert_model_simmilarities(udd_oecon_bert, jobindex_bert_oecon)\n",
    "arr_psych_2 = bert_model_simmilarities(udd_psych_bert, jobindex_bert_psych)\n",
    "arr_pol_2 = bert_model_simmilarities(udd_pol_bert, jobindex_bert_pol)\n",
    "arr_soc_2 = bert_model_simmilarities(udd_soc_bert, jobindex_bert_soc)\n",
    "arr_anth_2 = bert_model_simmilarities(udd_anth_bert, jobindex_bert_anth)\n",
    "\n",
    "_bert = (arr_oecon_2, arr_psych_2, arr_pol_2, arr_soc_2, arr_anth_2)\n",
    "df_bert_2 = pd.DataFrame(_bert)\n",
    "\n",
    "df_bert_2 = df_bert_2.rename(index={0:'cand.oecon', 1:'cand.psych', 2:'cand.scient.pol', 3:'cand.scient.soc', 4:'cand.scient.anth'})\n",
    "df_bert_2.columns=['Bert_2: Cosine Similarity']\n",
    "df_bert = pd.concat([df_bert, df_bert_2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_bert, df_tfidf], axis=1)\n",
    "df = pd.concat([df, df_spacy], axis=1)\n",
    "print(df.to_latex(index = True, multirow = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(jobindex_oecon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(jobindex_psych)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(jobindex_pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(jobindex_soc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(jobindex_anth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(udd_oecon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(udd_psych_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(udd_pol_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(udd_soc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot(udd_anth_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
