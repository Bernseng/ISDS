{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for data scrabing on Esco and Jobnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !python -m nltk.downloader popular\n",
    "# !pip install tensorflow\n",
    "# !pip install spacy\n",
    "# !pip install wordloud\n",
    "# !python -m spacy download da_core_news_md\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# from tensorflow.keras import layers\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "\n",
    "import string\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = nltk.corpus.stopwords.words('danish')\n",
    "nlp = spacy.load('da_core_news_md')\n",
    "output = Path(r'Exam/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data on Jobindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Empty\n",
    "\n",
    "def extract_jobindex(tag, pages):\n",
    "    flat_list = []\n",
    "    url_list = []\n",
    "    total_pages = range(pages)\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "\n",
    "    for page in total_pages:\n",
    "        url = f\"https://www.jobindex.dk/jobsoegning?maxdate=20220101&mindate=20210101&page={page}&jobage=archive&q={tag}\"\n",
    "                        \n",
    "        r = requests.get(url, headers)\n",
    "            \n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "                        \n",
    "        divs = soup.find_all(\"div\", class_=\"jobsearch-result\")\n",
    "        \n",
    "        for item in divs:\n",
    "            try:\n",
    "                job_url = item.select_one('[data-click*=\"u=\"]:has(> b)')['data-click']\n",
    "                url_list.append(job_url)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for i in url_list:\n",
    "            link = 'http://www.jobindex.dk' + i\n",
    "            flat_list.append(link)\n",
    "\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Jobindex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "porter = nltk.PorterStemmer()\n",
    "# nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jobindex(search_word, pages):\n",
    "    final_job = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "    final_list = []\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "    job = extract_jobindex(search_word, pages)\n",
    "    \n",
    "\n",
    "    for i in job:\n",
    "        url = f\"{i}\"\n",
    "        r = requests.get(url, headers)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "        iframe = soup.find_all('iframe', class_='archive-content')\n",
    "        for i in iframe:\n",
    "            link = i['src']\n",
    "            info_job.append(link)\n",
    "\n",
    "    for item in info_job:\n",
    "        links = 'http://www.jobindexarkiv.dk/cgi/showarchive.cgi' + item\n",
    "        jobindex.append(links)\n",
    "\n",
    "\n",
    "    for i in jobindex:\n",
    "        url_ = f\"{i}\"\n",
    "        r = requests.get(url_, headers)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "        content = soup.find_all('body')\n",
    "        for i in content:\n",
    "            text = i.get_text()\n",
    "            flat_list.append(text)\n",
    "\n",
    "    job_ = ' '.join(str(i) for i in flat_list)\n",
    "    _job = nltk.word_tokenize(clean_text(job_))\n",
    "    for word in _job:\n",
    "        if word not in stop_word:\n",
    "            final_job.append(word)\n",
    "            \n",
    "    return final_job\n",
    "\n",
    "# jobindex_psych = [wnl.lemmatize(i) for i in clean_jobindex('cand.psych')]\n",
    "jobindex_oecon = [wnl.lemmatize(i) for i in clean_jobindex('cand.oecon', 1)]\n",
    "# jobindex_pol = [wnl.lemmatize(i) for i in clean_jobindex('cand.scient.pol')]\n",
    "# jobindex_anth = [wnl.lemmatize(i) for i in clean_jobindex('cand.scient.anth')]\n",
    "# jobindex_soc = [wnl.lemmatize(i) for i in clean_jobindex('cand.scient.soc')]\n",
    "\n",
    "# psych = ' '.join(jobindex_psych)\n",
    "oecon = ' '.join(jobindex_oecon)\n",
    "# pol = ' '.join(jobindex_pol)\n",
    "# anth = ' '.join(jobindex_anth)\n",
    "# soc = ' '.join(jobindex_soc)\n",
    "\n",
    "# jobindex_oecon = clean_jobindex('cand.oecon', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing data using TfidfVectoriser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer.fit(jobindex_oecon)\n",
    "\n",
    "vector_jobindex = tfidfvectorizer.transform(jobindex_oecon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Jobindex data using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text('cand.oecon', jobindex_oecon)\n",
    "\n",
    "def stacy_words(input):\n",
    "    document = nlp(open(f'{input}', encoding=\"utf-8\").read())\n",
    "    adjs = []\n",
    "    for token in document:\n",
    "       if token.pos_ == 'ADJ':\n",
    "        adjs.append(token.lemma_)\n",
    "    # adjs_tally = Counter(adjs)\n",
    "    # adjs_tally.most_common()\n",
    "        adj = ' '.join(adjs)\n",
    "    return adj\n",
    "\n",
    "oecon_adjs = stacy_words('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS/Exam/cand.oecon.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "stopwords.update(['hr','bringe','ppr','vores','ønsker','nye','hverdag','del','inden','søger', \n",
    "'kan','egedal','oktober','kompetencer','li','ul','text','align','justify','br','p','div','strong',\n",
    "'style','href','både','ung','halsnæs','egen','daglig','hel','vigtig','relevant','velkommen','tidlig','første','enkelt','tæt','flest',\n",
    "'høj','ny','fast','mulig','lille','øvrig','mange','meget','central','voksen','stor','gammel','konkret','aktuel','ugenligt','gavn','formår',\n",
    "'god','vant','int','fig','centralt','evalueres','mittag','løbende','spise','nær','nemmere'])\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, max_font_size=35, max_words=100, background_color='white').generate(oecon_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stopwords, max_font_size=35, max_words=100, background_color='white').generate(oecon)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'økonom':'cand.oecon', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecon_esco = jobs.loc[jobs['job_title'] == 'cand.oecon']\n",
    "search_words = []\n",
    "for value in oecon_esco['optional_skill']:\n",
    "    search_words.append(value)\n",
    "    \n",
    "search_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecon.find('markedet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = nltk.word_tokenize(extract_UG(education_url[0][0]))\n",
    "oecon = nltk.word_tokenize(extract_UG(education_url[1][0]))\n",
    "pol = nltk.word_tokenize(extract_UG(education_url[2][0]))\n",
    "anth = nltk.word_tokenize(extract_UG(education_url[3][0]))\n",
    "soc = nltk.word_tokenize(extract_UG(education_url[4][0]))\n",
    "psych_final = []\n",
    "for word in psych:\n",
    "    if word not in stop_word:\n",
    "        psych_final.append(word)\n",
    "\n",
    "psych_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_text('psych', psych_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_text = tf.data.TextLineDataset('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS_edit/Exam/psych.txt').filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(psych_text.batch(4096))\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = psych_text.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in sequences[:10]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ku_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'statskundskab', 'antropologi', 'oekonomi']\n",
    "\n",
    "\n",
    "for k in search_list:\n",
    "    soup = extract_ku(k)\n",
    "    ku_list.append(transform_ku(soup))\n",
    "\n",
    "ku_df = pd.DataFrame(data=ku_list)\n",
    "\n",
    "#Merging the two colums \n",
    "ku_df=ku_df[0]+ku_df[1]\n",
    "\n",
    "#Making the object a dataframe\n",
    "ku_df=pd.DataFrame(ku_df)\n",
    "\n",
    "ku_df=pd.DataFrame.transpose(ku_df)\n",
    "\n",
    "ku_df.columns=['cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', 'cand.oecon_ku']\n",
    "soup = extract_au('statskundskab')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_stats = text[75:77]\n",
    "\n",
    "stats_df = pd.DataFrame(data=text_stats, columns=['cand.scient.pol_au'])\n",
    "stats = pd.DataFrame([', '.join(stats_df['cand.scient.pol_au'].to_list())], columns=['cand.scient.pol_au'])\n",
    "soup = extract_au('oekonomi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_oek = text[60:62]\n",
    "\n",
    "oek_df = pd.DataFrame(data=text_oek, columns=['cand.oecon_au'])\n",
    "oek = pd.DataFrame([', '.join(oek_df['cand.oecon_au'].to_list())], columns=['cand.oecon_au'])\n",
    "soup = extract_au('antropologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_ant = text[50:55]\n",
    "\n",
    "ant_df = pd.DataFrame(data=text_ant, columns=['cand.scient.anth_au'])\n",
    "ant = pd.DataFrame([', '.join(ant_df['cand.scient.anth_au'].to_list())], columns=['cand.scient.anth_au'])\n",
    "soup = extract_au('psykologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_psyk = text[74:78]\n",
    "\n",
    "psyk_df = pd.DataFrame(data=text_psyk, columns=['cand.psych_au'])\n",
    "psyk = pd.DataFrame([', '.join(psyk_df['cand.psych_au'].to_list())], columns=['cand.psych_au'])\n",
    "frames = [ant, stats, psyk, oek]\n",
    "\n",
    "au_df = pd.concat(frames, axis=1)\n",
    "\n",
    "au_df_list = au_df['cand.scient.anth_au'].to_list()\n",
    "au_list = []\n",
    "\n",
    "for i in au_df_list:\n",
    "    au_list.append(clean_text(i))\n",
    "\n",
    "au_list\n",
    "def transform_aau(soup):\n",
    "\n",
    "    divs = soup.find_all(\"main\", class_=\"Main_Main__2KIvG\")\n",
    "\n",
    "    for item in divs:\n",
    "        text_aau = item.find_all()[0].text.strip()\n",
    "\n",
    "        aau_text = {\n",
    "            \"text_aau\" : text_aau, \n",
    "        }\n",
    "        aau_list.append(aau_text)\n",
    "\n",
    "        text_aau = clean_text(text_aau)\n",
    "        \n",
    "    return text_aau\n",
    "\n",
    "aau_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    try: \n",
    "        soup = extract_aau(k)\n",
    "        transform_aau(soup)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "aau_df = pd.DataFrame(data=aau_list)\n",
    "\n",
    "aau_df=pd.DataFrame.transpose(aau_df)\n",
    "\n",
    "aau_df.columns=['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau']\n",
    "\n",
    "aau_df = aau_df.reset_index(drop=True)\n",
    "\n",
    "aau_df\n",
    "\n",
    "## Combining Dataframes\n",
    "merge_frames = [ku_df, au_df, aau_df]\n",
    "\n",
    "combined_df = pd.concat(merge_frames, axis=1)\n",
    "\n",
    "combined_df['cand.psych_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.psych_ku']]\n",
    "combined_df['cand.scient.pol_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.pol_ku']]\n",
    "combined_df['cand.oecon_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.oecon_ku']]\n",
    "combined_df['cand.scient.anth_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.anth_ku']]\n",
    "combined_df['cand.scient.soc_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.soc_ku']]\n",
    "\n",
    "combined_df['cand.psych'] = combined_df['cand.psych_aau'] + combined_df['cand.psych_au'] + combined_df['cand.psych_ku_string']\n",
    "combined_df['cand.scient.anth'] = combined_df['cand.scient.anth_au'] + combined_df['cand.scient.anth_ku_string']\n",
    "combined_df['cand.scient.pol'] = combined_df['cand.scient.pol_au'] + combined_df['cand.scient.pol_ku_string']\n",
    "combined_df['cand.scient.soc'] = combined_df['cand.scient.soc_aau'] + combined_df['cand.scient.soc_ku_string']\n",
    "combined_df['cand.oecon'] = combined_df['cand.oecon_aau'] + combined_df['cand.oecon_au'] + combined_df['cand.oecon_ku_string']\n",
    "combined_df.drop(['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau', 'cand.psych_au', \\\n",
    "                  'cand.scient.anth_au', 'cand.scient.pol_au','cand.oecon_au', \\\n",
    "                  'cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', \\\n",
    "                  'cand.oecon_ku', 'cand.psych_ku_string', 'cand.scient.pol_ku_string', 'cand.oecon_ku_string', \\\n",
    "                  'cand.scient.anth_ku_string', 'cand.scient.soc_ku_string'], axis=1, inplace=True)\n",
    "university_df = combined_df\n",
    "university_df\n",
    "## Scraping UG\n",
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]\n",
    "psych = clean_text(extract_UG(education_url[0][0]))\n",
    "oecon = extract_UG(education_url[1][0])\n",
    "pol = extract_UG(education_url[2][0])\n",
    "anth = extract_UG(education_url[3][0])\n",
    "soc = extract_UG(education_url[4][0])\n",
    "## Combining skills from UG and Universities\n",
    "strings_psych = psych + university_df['cand.psych']\n",
    "strings_oecon = oecon + university_df['cand.oecon']\n",
    "strings_pol = pol + university_df['cand.scient.pol']\n",
    "strings_anth = anth + university_df['cand.scient.anth']\n",
    "strings_soc = soc + university_df['cand.scient.soc']\n",
    "\n",
    "\n",
    "psych_comb = \" \".join(strings_psych)\n",
    "oecon_comb = \" \".join(strings_oecon)\n",
    "pol_comb = \" \".join(strings_pol)\n",
    "anth_comb = \" \".join(strings_anth)\n",
    "soc_comb = \" \".join(strings_soc)\n",
    "psych_series = pd.Series(psych_comb)\n",
    "oecon_series = pd.Series(oecon_comb)\n",
    "pol_series = pd.Series(pol_comb)\n",
    "anth_series = pd.Series(anth_comb)\n",
    "soc_series = pd.Series(soc_comb)\n",
    "\n",
    "psych_df = pd.DataFrame(psych_series)\n",
    "oecon_df = pd.DataFrame(oecon_series)\n",
    "pol_df = pd.DataFrame(pol_series)\n",
    "anth_df = pd.DataFrame(anth_series)\n",
    "soc_df = pd.DataFrame(soc_series)\n",
    "\n",
    "psych_df['cand.psych'] = psych_df[0]\n",
    "oecon_df['cand.oecon'] = oecon_df[0]\n",
    "pol_df['cand.scient.pol'] = pol_df[0]\n",
    "anth_df['cand.scient.anth'] = anth_df[0]\n",
    "soc_df['cand.scient.soc'] = soc_df[0]\n",
    "final_df = pd.concat([psych_df['cand.psych'], oecon_df['cand.oecon'], pol_df['cand.scient.pol'], \\\n",
    "                      anth_df['cand.scient.anth'], soc_df['cand.scient.soc']], axis=1)\n",
    "\n",
    "final_df = make_a_list(final_df['cand.psych']).append(make_a_list(final_df['cand.oecon'])).append(make_a_list(final_df['cand.scient.pol']))\\\n",
    "    .append(make_a_list(final_df['cand.scient.anth'])).append(make_a_list(final_df['cand.scient.soc']))\n",
    "\n",
    "final_df = final_df.T\n",
    "final_df.columns = ['cand.psych', 'cand_oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "final_df\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(nltk.word_tokenize)\n",
    "final_df['cand_oecon'] = final_df['cand_oecon'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(nltk.word_tokenize)\n",
    "# stop = stopwords.words('danish')\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand_oecon'] = final_df['cand_oecon'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "# final_df.to_csv('out.csv')\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['cand_oecon'] = ''.join(' '.join(i) for i in final_df.cand_oecon)\n",
    "for i in final_df['cand_oecon']:\n",
    "    oecon = list(i)\n",
    "oecon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape () instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/4k8gxvgx5xl9x753jsnhn1_r0000gn/T/ipykernel_8702/3656833328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlr_regression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Text classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlr_regression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_jobindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_udd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# pairwise_similarities = np.dot(vector_jobindex, vector_udd).toarray()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[1;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    881\u001b[0m                         ensure_2d=False, dtype=None)\n\u001b[1;32m    882\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;34m\"y should be a 1d array, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m         \"got an array of shape {} instead.\".format(shape))\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
     ]
    }
   ],
   "source": [
    "# jobindex = tfidfvectorizer.fit(jobindex_oecon)\n",
    "\n",
    "# udd = tfidfvectorizer.fit(oecon)\n",
    "\n",
    "vector_udd = tfidfvectorizer.fit_transform(oecon)\n",
    "\n",
    "vector_jobindex = tfidfvectorizer.fit_transform(jobindex_oecon)\n",
    "\n",
    "preds = vector_jobindex.predict()\n",
    "print(\"Testing accuracy =\",np.mean([(preds==tweet_sentiments)]))\n",
    "\n",
    "lr_regression = LogisticRegression(random_state=0) #Text classifier\n",
    "\n",
    "lr_regression.fit(vector_jobindex, vector_udd)\n",
    "\n",
    "# pairwise_similarities = np.dot(vector_jobindex, vector_udd).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
