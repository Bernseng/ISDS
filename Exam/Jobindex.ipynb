{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for data scrabing on Esco and Jobnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !python -m nltk.downloader popular\n",
    "# !pip install tensorflow\n",
    "# !pip install spacy\n",
    "# !pip install wordloud\n",
    "# !python -m spacy download da_core_news_md\n",
    "# !pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tensorflow.keras import layers\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "\n",
    "import string\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import lemmy\n",
    "lemmatizer = lemmy.load(\"da\")\n",
    "\n",
    "from function import *\n",
    "\n",
    "seed = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopord.txt', encoding='utf8') as f:\n",
    "    stopord = f.read().splitlines()\n",
    "stop_word = nltk.corpus.stopwords.words('danish') + stopord\n",
    "\n",
    "nlp = spacy.load('da_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data on Jobindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Empty\n",
    "\n",
    "def extract_jobindex(tag, pages):\n",
    "    flat_list = []\n",
    "    url_list = []\n",
    "    total_pages = range(pages)\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "\n",
    "    for page in total_pages:\n",
    "        url = f\"https://www.jobindex.dk/jobsoegning?maxdate=20220101&mindate=20210101&page={page}&jobage=archive&q={tag}\"\n",
    "                        \n",
    "        r = requests.get(url, headers)\n",
    "            \n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "                        \n",
    "        divs = soup.find_all(\"div\", class_=\"jobsearch-result\")\n",
    "        \n",
    "        for item in divs:\n",
    "            try:\n",
    "                job_url = item.select_one('[data-click*=\"u=\"]:has(> b)')['data-click']\n",
    "                url_list.append(job_url)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for i in url_list:\n",
    "            link = 'http://www.jobindex.dk' + i\n",
    "            flat_list.append(link)\n",
    "\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Jobindex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "# nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word in \n",
      " psych: 672049, \n",
      " oecon: 1041890, \n",
      " pol: 1053568, \n",
      " anth: 206842, \n",
      " soc: 518072\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(input):\n",
    "    list_ = []\n",
    "    func = [lemmatizer.lemmatize('', i) for i in input]\n",
    "    for sublist in func:\n",
    "        list_.append(sublist[:1])\n",
    "\n",
    "    return list_\n",
    "\n",
    "def clean_jobindex(search_word):\n",
    "    final_job = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "    final_list = []\n",
    "    job_list = []\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "    job = extract_jobindex(search_word, 4)\n",
    "    \n",
    "    for i in job:\n",
    "        url = f\"{i}\"\n",
    "        r = requests.get(url, headers)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "        iframe = soup.find_all('iframe', class_='archive-content')\n",
    "        for i in iframe:\n",
    "            link = i['src']\n",
    "            info_job.append(link)\n",
    "\n",
    "    for item in info_job:\n",
    "        links = 'http://www.jobindexarkiv.dk/cgi/showarchive.cgi' + item\n",
    "        jobindex.append(links)\n",
    "\n",
    "\n",
    "    for i in jobindex:\n",
    "        url_ = f\"{i}\"\n",
    "        r = requests.get(url_, headers)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        content = soup.find_all('body')\n",
    "        for i in content:\n",
    "            text = i.get_text()\n",
    "            flat_list.append(text)\n",
    "\n",
    "    job_ = ' '.join(str(i) for i in flat_list)\n",
    "    _job = nltk.word_tokenize(clean_text(job_))\n",
    "    job__ = lemmatize_text(_job)\n",
    "    for word in job__:\n",
    "        if word not in stop_word:\n",
    "            final_job.append(word)\n",
    "\n",
    "    for sublist in final_job:\n",
    "        job_list.append(' '.join(sublist))\n",
    "\n",
    "    return job_list\n",
    "\n",
    "\n",
    "jobindex_psych = clean_jobindex('cand.psych')\n",
    "jobindex_oecon = clean_jobindex('cand.oecon')\n",
    "jobindex_pol = clean_jobindex('cand.scient.pol')\n",
    "jobindex_anth = clean_jobindex('cand.scient.anth')\n",
    "jobindex_soc = clean_jobindex('cand.scient.soc')\n",
    "\n",
    "jobindex_psych_full = ' '.join(jobindex_psych)\n",
    "jobindex_oecon_full = ' '.join(jobindex_oecon)\n",
    "jobindex_pol_full = ' '.join(jobindex_pol)\n",
    "jobindex_anth_full = ' '.join(jobindex_anth)\n",
    "jobindex_soc_full = ' '.join(jobindex_soc)\n",
    "\n",
    "print(f'Number of word in \\n psych: {len(jobindex_psych_full)}, \\n oecon: {len(jobindex_oecon_full)}, \\n pol: {len(jobindex_pol_full)}, \\n anth: {len(jobindex_anth_full)}, \\n soc: {len(jobindex_soc_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Jobindex data using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text('cand.oecon', jobindex_oecon)\n",
    "write_text('cand.psych', jobindex_psych)\n",
    "write_text('cand.pol', jobindex_pol)\n",
    "write_text('cand.anth', jobindex_anth)\n",
    "write_text('cand.soc', jobindex_soc)\n",
    "\n",
    "def stacy_words(input):\n",
    "    document = nlp(open(f'{input}', encoding=\"utf-8\").read())\n",
    "    adjs = []\n",
    "    for token in document:\n",
    "       if token.pos_ == 'ADJ':\n",
    "        adjs.append(token.lemma_)\n",
    "    # adjs_tally = Counter(adjs)\n",
    "    # adjs_tally.most_common()\n",
    "        adj = ' '.join(adjs)\n",
    "    return adj\n",
    "\n",
    "path = Path('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS/Exam/')\n",
    "\n",
    "oecon_adjs = stacy_words(path / 'cand.oecon.txt')\n",
    "psych_adjs = stacy_words(path / 'cand.psych.txt')\n",
    "pol_adjs = stacy_words(path / 'cand.pol.txt')\n",
    "anth_adjs = stacy_words(path / 'cand.anth.txt')\n",
    "soc_adjs = stacy_words(path / 'cand.soc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(oecon_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(psych_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(pol_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(anth_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(soc_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'økonom':'cand.oecon', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_search_words(df, type_):\n",
    "    search_words = []\n",
    "    searchword = df.loc[jobs['job_title'] == f'{type_}']\n",
    "    for value in searchword['optional_skill']:\n",
    "        search_words.append(value)\n",
    "    for value in searchword['essential_skill']:\n",
    "        search_words.append(value)\n",
    "    return search_words\n",
    "\n",
    "esco_oecon = find_search_words(jobs, 'cand.oecon')\n",
    "esco_psych = find_search_words(jobs, 'cand.psych')\n",
    "esco_pol = find_search_words(jobs, 'cand.scient.pol')\n",
    "esco_anth = find_search_words(jobs, 'cand.scient.anth')\n",
    "esco_soc = find_search_words(jobs, 'cand.scient.soc')\n",
    "\n",
    "print(f'Number of skills in: \\n psych: {len(esco_psych)}, \\n oecon: {len(esco_oecon)}, \\\n",
    "    \\n pol: {len(esco_pol)}, \\n anth: {len(esco_anth)}, \\n soc: {len(esco_soc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psych = nltk.word_tokenize(extract_UG(education_url[0][0]))\n",
    "# oecon = nltk.word_tokenize(extract_UG(education_url[1][0]))\n",
    "# pol = nltk.word_tokenize(extract_UG(education_url[2][0]))\n",
    "# anth = nltk.word_tokenize(extract_UG(education_url[3][0]))\n",
    "# soc = nltk.word_tokenize(extract_UG(education_url[4][0]))\n",
    "# psych_final = []\n",
    "# for word in psych:\n",
    "#     if word not in stop_word:\n",
    "#         psych_final.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ku_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'statskundskab', 'antropologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    soup = extract_ku(k)\n",
    "    ku_list.append(transform_ku(soup))\n",
    "\n",
    "ku_df = pd.DataFrame(data=ku_list)\n",
    "\n",
    "#Merging the two colums \n",
    "ku_df=ku_df[0]+ku_df[1]\n",
    "\n",
    "#Making the object a dataframe\n",
    "ku_df=pd.DataFrame(ku_df)\n",
    "\n",
    "ku_df=pd.DataFrame.transpose(ku_df)\n",
    "\n",
    "ku_df.columns=['cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', 'cand.oecon_ku']\n",
    "soup = extract_au('statskundskab')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_stats = text[75:77]\n",
    "\n",
    "stats_df = pd.DataFrame(data=text_stats, columns=['cand.scient.pol_au'])\n",
    "stats = pd.DataFrame([', '.join(stats_df['cand.scient.pol_au'].to_list())], columns=['cand.scient.pol_au'])\n",
    "soup = extract_au('oekonomi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_oek = text[60:62]\n",
    "\n",
    "oek_df = pd.DataFrame(data=text_oek, columns=['cand.oecon_au'])\n",
    "oek = pd.DataFrame([', '.join(oek_df['cand.oecon_au'].to_list())], columns=['cand.oecon_au'])\n",
    "soup = extract_au('antropologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_ant = text[50:55]\n",
    "\n",
    "ant_df = pd.DataFrame(data=text_ant, columns=['cand.scient.anth_au'])\n",
    "ant = pd.DataFrame([', '.join(ant_df['cand.scient.anth_au'].to_list())], columns=['cand.scient.anth_au'])\n",
    "soup = extract_au('psykologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_psyk = text[74:78]\n",
    "\n",
    "psyk_df = pd.DataFrame(data=text_psyk, columns=['cand.psych_au'])\n",
    "psyk = pd.DataFrame([', '.join(psyk_df['cand.psych_au'].to_list())], columns=['cand.psych_au'])\n",
    "frames = [ant, stats, psyk, oek]\n",
    "\n",
    "au_df = pd.concat(frames, axis=1)\n",
    "\n",
    "au_df_list = au_df['cand.scient.anth_au'].to_list()\n",
    "au_list = []\n",
    "\n",
    "for i in au_df_list:\n",
    "    au_list.append(clean_text(i))\n",
    "\n",
    "au_list\n",
    "def transform_aau(soup):\n",
    "\n",
    "    divs = soup.find_all(\"main\", class_=\"Main_Main__2KIvG\")\n",
    "\n",
    "    for item in divs:\n",
    "        text_aau = item.find_all()[0].text.strip()\n",
    "\n",
    "        aau_text = {\n",
    "            \"text_aau\" : text_aau, \n",
    "        }\n",
    "        aau_list.append(aau_text)\n",
    "\n",
    "        text_aau = clean_text(text_aau)\n",
    "        \n",
    "    return text_aau\n",
    "\n",
    "aau_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    try: \n",
    "        soup = extract_aau(k)\n",
    "        transform_aau(soup)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "aau_df = pd.DataFrame(data=aau_list)\n",
    "\n",
    "aau_df=pd.DataFrame.transpose(aau_df)\n",
    "\n",
    "aau_df.columns=['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau']\n",
    "\n",
    "aau_df = aau_df.reset_index(drop=True)\n",
    "\n",
    "aau_df\n",
    "\n",
    "## Combining Dataframes\n",
    "merge_frames = [ku_df, au_df, aau_df]\n",
    "\n",
    "combined_df = pd.concat(merge_frames, axis=1)\n",
    "\n",
    "combined_df['cand.psych_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.psych_ku']]\n",
    "combined_df['cand.scient.pol_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.pol_ku']]\n",
    "combined_df['cand.oecon_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.oecon_ku']]\n",
    "combined_df['cand.scient.anth_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.anth_ku']]\n",
    "combined_df['cand.scient.soc_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.soc_ku']]\n",
    "\n",
    "combined_df['cand.psych'] = combined_df['cand.psych_aau'] + combined_df['cand.psych_au'] + combined_df['cand.psych_ku_string']\n",
    "combined_df['cand.scient.anth'] = combined_df['cand.scient.anth_au'] + combined_df['cand.scient.anth_ku_string']\n",
    "combined_df['cand.scient.pol'] = combined_df['cand.scient.pol_au'] + combined_df['cand.scient.pol_ku_string']\n",
    "combined_df['cand.scient.soc'] = combined_df['cand.scient.soc_aau'] + combined_df['cand.scient.soc_ku_string']\n",
    "combined_df['cand.oecon'] = combined_df['cand.oecon_aau'] + combined_df['cand.oecon_au'] + combined_df['cand.oecon_ku_string']\n",
    "combined_df.drop(['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau', 'cand.psych_au', \\\n",
    "                  'cand.scient.anth_au', 'cand.scient.pol_au','cand.oecon_au', \\\n",
    "                  'cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', \\\n",
    "                  'cand.oecon_ku', 'cand.psych_ku_string', 'cand.scient.pol_ku_string', 'cand.oecon_ku_string', \\\n",
    "                  'cand.scient.anth_ku_string', 'cand.scient.soc_ku_string'], axis=1, inplace=True)\n",
    "university_df = combined_df\n",
    "university_df\n",
    "\n",
    "## Scraping UG\n",
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]\n",
    "psych = clean_text(extract_UG(education_url[0][0]))\n",
    "oecon = extract_UG(education_url[1][0])\n",
    "pol = extract_UG(education_url[2][0])\n",
    "anth = extract_UG(education_url[3][0])\n",
    "soc = extract_UG(education_url[4][0])\n",
    "\n",
    "## Combining skills from UG and Universities\n",
    "strings_psych = psych + university_df['cand.psych']\n",
    "strings_oecon = oecon + university_df['cand.oecon']\n",
    "strings_pol = pol + university_df['cand.scient.pol']\n",
    "strings_anth = anth + university_df['cand.scient.anth']\n",
    "strings_soc = soc + university_df['cand.scient.soc']\n",
    "\n",
    "\n",
    "psych_comb = \" \".join(strings_psych)\n",
    "oecon_comb = \" \".join(strings_oecon)\n",
    "pol_comb = \" \".join(strings_pol)\n",
    "anth_comb = \" \".join(strings_anth)\n",
    "soc_comb = \" \".join(strings_soc)\n",
    "psych_series = pd.Series(psych_comb)\n",
    "oecon_series = pd.Series(oecon_comb)\n",
    "pol_series = pd.Series(pol_comb)\n",
    "anth_series = pd.Series(anth_comb)\n",
    "soc_series = pd.Series(soc_comb)\n",
    "\n",
    "psych_df = pd.DataFrame(psych_series)\n",
    "oecon_df = pd.DataFrame(oecon_series)\n",
    "pol_df = pd.DataFrame(pol_series)\n",
    "anth_df = pd.DataFrame(anth_series)\n",
    "soc_df = pd.DataFrame(soc_series)\n",
    "\n",
    "psych_df['cand.psych'] = psych_df[0]\n",
    "oecon_df['cand.oecon'] = oecon_df[0]\n",
    "pol_df['cand.scient.pol'] = pol_df[0]\n",
    "anth_df['cand.scient.anth'] = anth_df[0]\n",
    "soc_df['cand.scient.soc'] = soc_df[0]\n",
    "final_df = pd.concat([psych_df['cand.psych'], oecon_df['cand.oecon'], pol_df['cand.scient.pol'], \\\n",
    "                      anth_df['cand.scient.anth'], soc_df['cand.scient.soc']], axis=1)\n",
    "\n",
    "final_df = make_a_list(final_df['cand.psych']).append(make_a_list(final_df['cand.oecon'])).append(make_a_list(final_df['cand.scient.pol']))\\\n",
    "    .append(make_a_list(final_df['cand.scient.anth'])).append(make_a_list(final_df['cand.scient.soc']))\n",
    "\n",
    "final_df = final_df.T\n",
    "final_df.columns = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(nltk.word_tokenize)\n",
    "final_df['cand.oecon'] = final_df['cand.oecon'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(nltk.word_tokenize)\n",
    "\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.oecon'] = final_df['cand.oecon'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(lambda words: [word for word in words if word not in stop_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(series):\n",
    "    final_list_ = []\n",
    "    for i in series:\n",
    "        list_ = list(i)\n",
    "    list_ = ' '.join(list_)\n",
    "    final_list = list_.split(' ')\n",
    "    for word in final_list:\n",
    "        if word not in stop_word:\n",
    "            final_list_.append(word)\n",
    "    return final_list_\n",
    "\n",
    "udd_oecon_list = df_to_list(final_df['cand.oecon'])\n",
    "udd_psych_list = df_to_list(final_df['cand.psych'])\n",
    "udd_pol_list = df_to_list(final_df['cand.scient.pol'])\n",
    "udd_anth_list = df_to_list(final_df['cand.scient.anth'])\n",
    "udd_soc_list = df_to_list(final_df['cand.scient.soc'])\n",
    "\n",
    "udd_oecon_string = ' '.join(udd_oecon_list)\n",
    "udd_psych_string = ' '.join(udd_psych_list)\n",
    "udd_anth_string = ' '.join(udd_anth_list)\n",
    "udd_pol_string = ' '.join(udd_pol_list)\n",
    "udd_soc_string = ' '.join(udd_soc_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n",
    "def tensorflow_training_data(input):\n",
    "\n",
    "  text_ds = tf.data.TextLineDataset(f'{input}')\n",
    "\n",
    "  BATCH_SIZE = 30000\n",
    "\n",
    "  vocab_size = BATCH_SIZE\n",
    "\n",
    "  sequence_length = 20\n",
    "\n",
    "  def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    return tf.strings.regex_replace(lowercase,\n",
    "                                    '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "  # Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "  # integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "  # same length.\n",
    "  vectorize_layer = layers.TextVectorization(\n",
    "      standardize=custom_standardization,\n",
    "      max_tokens=vocab_size,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=sequence_length,\n",
    "      pad_to_max_tokens=False,\n",
    "      ngrams=None,\n",
    "      vocabulary=None)\n",
    "\n",
    "  vectorize_layer.adapt(text_ds.batch(BATCH_SIZE))\n",
    "  inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "  #Vectorize data\n",
    "  text_vector_ds = text_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "  sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "  print(len(sequences))\n",
    "  for seq in sequences[:10]:\n",
    "    print(f'{seq} => {[inverse_vocab[i] for i in seq]}')\n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n",
    "\n",
    "tensorflow_training_data('cand.pol.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(words_to_search, list_, name):\n",
    "    for i in words_to_search:\n",
    "        if i in list_:\n",
    "            print(f'{i} is present in the list of {name}')\n",
    "\n",
    "find_words(esco_oecon, jobindex_oecon, 'oecon')\n",
    "find_words(esco_anth, jobindex_anth, 'anth')\n",
    "find_words(esco_pol, jobindex_pol, 'pol')\n",
    "find_words(esco_psych, jobindex_psych, 'psych')\n",
    "find_words(esco_soc, jobindex_soc, 'soc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectorizer(input_udd, input_jobindex):\n",
    "    list_ = []\n",
    "    _list = []\n",
    "    vector = TfidfVectorizer(lowercase=False)\n",
    "    analyzer = ([input_udd] + [input_jobindex]) #The order does not make a difference here\n",
    "    ret = vector.fit_transform(analyzer)\n",
    "    pairwise_similarity = ret * ret.T\n",
    "    array = pairwise_similarity.toarray()\n",
    "    for cell in np.nditer(array):\n",
    "        if cell < 0.9 and cell > 0.1:\n",
    "            list_.append(cell)\n",
    "    \n",
    "    \n",
    "    return list_[1].tolist()\n",
    "\n",
    "print(f'oecon: {make_vectorizer(udd_oecon_string, jobindex_oecon_full)} \\\n",
    "anth: {make_vectorizer(udd_anth_string, jobindex_anth_full)} \\\n",
    "soc: {make_vectorizer(udd_soc_string, jobindex_soc_full)} \\\n",
    "pol: {make_vectorizer(udd_pol_string, jobindex_pol_full)} \\\n",
    "psych: {make_vectorizer(udd_psych_string, jobindex_psych_full)}')\n",
    "\n",
    "df_tfidf = pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similarities with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecon_udd_nlp = nlp(udd_oecon_string)\n",
    "soc_udd_nlp = nlp(udd_soc_string)\n",
    "anth_udd_nlp = nlp(udd_anth_string)\n",
    "psych_udd_nlp = nlp(udd_psych_string)\n",
    "pol_udd_nlp = nlp(udd_pol_string)\n",
    "\n",
    "oecon_job_nlp = nlp(jobindex_oecon_full)\n",
    "soc_job_nlp = nlp(jobindex_soc_full)\n",
    "anth_job_nlp = nlp(jobindex_anth_full)\n",
    "psych_job_nlp = nlp(jobindex_psych_full)\n",
    "pol_job_nlp = nlp(jobindex_pol_full)\n",
    "\n",
    "print(f'cand.oecon similarity: {oecon_job_nlp.similarity(oecon_udd_nlp)} \\n \\\n",
    "cand.soc similarity: {soc_job_nlp.similarity(soc_udd_nlp)} \\n \\\n",
    "cand.anth similarity: {anth_job_nlp.similarity(anth_udd_nlp)} \\n \\\n",
    "cand.psych similarity: {psych_job_nlp.similarity(psych_udd_nlp)} \\n \\\n",
    "cand.pol similarity: {pol_job_nlp.similarity(pol_udd_nlp)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "def bert_model_simmilarities(udd_list, jobindex_list):\n",
    "    list_ = []\n",
    "    document_embeddings = sbert_model.encode([udd_list, jobindex_list])\n",
    "    similarities = cosine_similarity(document_embeddings)\n",
    "    for cell in np.nditer(similarities):\n",
    "        if cell < 0.9 and cell > 0.1:\n",
    "            list_.append(cell)\n",
    "\n",
    "    return list_[1].tolist()\n",
    "\n",
    "def bert_model_differences(udd_list, jobindex_list):\n",
    "    list_ = []\n",
    "    document_embeddings = sbert_model.encode([udd_list, jobindex_list])\n",
    "    differences = euclidean_distances(document_embeddings)\n",
    "\n",
    "    for cell in np.nditer(differences):\n",
    "        if cell > 0.1:\n",
    "            list_.append(cell)\n",
    "\n",
    "    return list_[1].tolist()\n",
    "\n",
    "arr_oecon = bert_model_differences(udd_oecon_list, jobindex_oecon)\n",
    "arr_oecon_2 = bert_model_simmilarities(udd_oecon_list, jobindex_oecon)\n",
    "\n",
    "arr_psych = bert_model_differences(udd_psych_list, jobindex_psych)\n",
    "arr_psych_2 = bert_model_simmilarities(udd_psych_list, jobindex_psych)\n",
    "\n",
    "arr_pol = bert_model_differences(udd_pol_list, jobindex_pol)\n",
    "arr_pol_2 = bert_model_simmilarities(udd_pol_list, jobindex_pol)\n",
    "\n",
    "arr_soc = bert_model_differences(udd_soc_list, jobindex_soc)\n",
    "arr_soc_2 = bert_model_simmilarities(udd_soc_list, jobindex_soc)\n",
    "\n",
    "arr_anth = bert_model_differences(udd_anth_list, jobindex_anth)\n",
    "arr_anth_2 = bert_model_simmilarities(udd_anth_list, jobindex_anth)\n",
    "\n",
    "list_ = (arr_oecon, arr_psych, arr_pol, arr_soc, arr_anth)\n",
    "_list = (arr_oecon_2, arr_psych_2, arr_pol_2, arr_soc_2, arr_anth_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = pd.DataFrame(list_)\n",
    "\n",
    "df_sim = pd.DataFrame(_list)\n",
    "\n",
    "df = pd.concat([df_diff, df_sim], axis=1)\n",
    "df = df.rename(index={0:'cand.oecon', 1:'cand.psych', 2:'cand.scient.pol', 3:'cand.scient.soc', 4:'cand.scient.anth'})\n",
    "df.columns=['Euclidean Distance', 'Cosine Similarity']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
