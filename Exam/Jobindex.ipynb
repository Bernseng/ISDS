{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for data scrabing on Esco and Jobnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !python -m nltk.downloader popular\n",
    "# !pip install tensorflow\n",
    "# !pip install spacy\n",
    "# !pip install wordloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "stop_word = nltk.corpus.stopwords.words('danish')\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "import string\n",
    "from function import *\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "output = Path(r'Exam/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data on Jobindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped looping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://egedalkommune.emply.net/recruitment/vacancyAd.aspx?publishingId=eb90adbc-80e1-4782-87f1-6b8da971182e',\n",
       " 'https://esbjerg.emply.net/recruitment/vacancyAd.aspx?publishingId=bab7ab09-09fd-4314-8744-a3242f84d6af',\n",
       " 'https://fredensborg.career.emply.com/ad/vi-soger-dig-som-psykolog-til-at-varetage-bade-familiebehandling-og-ppr-opgaver/c0ve9o/da',\n",
       " 'https://ringsted.career.emply.com/ad/psykologfaglig-kollega-til-ppr-cand-psych-cand-paed-psych-eller-lignende/7ptwrj/da',\n",
       " 'https://www.jobindex.dk/jobannonce/454075/hr-specialist-til-verdensfoerende-global-virksomhed',\n",
       " 'https://job.wilke.dk/kvalitativ-konsulent-med-god-forretningsforst-else-til-vores-kontor-i-k-benhavn-100016/',\n",
       " 'https://www.jobindex.dk/jobannonce/451458/psykologer-til-boerneterapi-supervision-og-behandlingsanalyse',\n",
       " 'https://halsnaes.career.emply.com/ad/ppr-soger-psykolog/r53r9k/da']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cmath import nan\n",
    "from queue import Empty\n",
    "\n",
    "\n",
    "def extract_jobindex(page, tag):\n",
    "    flat_list = []\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    url = f\"https://www.jobindex.dk/jobsoegning?page={page}&q={tag}\"\n",
    "                    \n",
    "    r = requests.get(url, headers)\n",
    "        \n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "                    \n",
    "    divs = soup.find_all(\"div\", class_=\"PaidJob-inner\")         \n",
    "    url_list = []\n",
    "\n",
    "    for item in divs:\n",
    "        # summary = item.find_all(\"p\")\n",
    "        job_url =  item.select_one('[data-click*=\"u=\"]:has(> b)')['href']\n",
    "\n",
    "        # job = ''.join([str(i) for i in summary])\n",
    "        url_list.append(job_url)\n",
    "\n",
    "        # job_list.append(job)\n",
    "\n",
    "    for sublist in url_list:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    \n",
    "    return url_list\n",
    "job = []\n",
    "list_ = [1,2,3]\n",
    "for i in list_:\n",
    "    job = extract_jobindex(i, 'cand.psych')\n",
    "    if extract_jobindex(i, 'cand.psych') != Empty:\n",
    "        print('stopped looping')\n",
    "        break\n",
    "        \n",
    "\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached maximum job articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://egedalkommune.emply.net/recruitment/vacancyAd.aspx?publishingId=eb90adbc-80e1-4782-87f1-6b8da971182e',\n",
       " 'https://esbjerg.emply.net/recruitment/vacancyAd.aspx?publishingId=bab7ab09-09fd-4314-8744-a3242f84d6af',\n",
       " 'https://fredensborg.career.emply.com/ad/vi-soger-dig-som-psykolog-til-at-varetage-bade-familiebehandling-og-ppr-opgaver/c0ve9o/da',\n",
       " 'https://ringsted.career.emply.com/ad/psykologfaglig-kollega-til-ppr-cand-psych-cand-paed-psych-eller-lignende/7ptwrj/da',\n",
       " 'https://www.jobindex.dk/jobannonce/454075/hr-specialist-til-verdensfoerende-global-virksomhed',\n",
       " 'https://job.wilke.dk/kvalitativ-konsulent-med-god-forretningsforst-else-til-vores-kontor-i-k-benhavn-100016/',\n",
       " 'https://www.jobindex.dk/jobannonce/451458/psykologer-til-boerneterapi-supervision-og-behandlingsanalyse',\n",
       " 'https://halsnaes.career.emply.com/ad/ppr-soger-psykolog/r53r9k/da']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_jobindex(search_word):\n",
    "    job_url = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    for i in range(5):\n",
    "        job = extract_jobindex(i, 'cand.psych')\n",
    "        if extract_jobindex(i, 'cand.psych') != Empty:\n",
    "            print('reached maximum job articles')\n",
    "            break\n",
    "\n",
    "    return job\n",
    "\n",
    "clean_jobindex('cand.psych')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Jobindex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached maximum job articles\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty separator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/4k8gxvgx5xl9x753jsnhn1_r0000gn/T/ipykernel_21281/841991096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mpsych\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mjobindex_psych\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobindex_psych\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mjob_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobindex_psych\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS/Exam/function.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;31m# text = text.replace('[','').replace(']', '')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty separator"
     ]
    }
   ],
   "source": [
    "search_list = ['cand.psych', 'cand.polit', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "def clean_jobindex(search_word):\n",
    "    job_url = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    for i in range(5):\n",
    "        job = extract_jobindex(i, 'cand.psych')\n",
    "        if extract_jobindex(i, 'cand.psych') != Empty:\n",
    "            print('reached maximum job articles')\n",
    "            break\n",
    "    \n",
    "    for i in job:\n",
    "        url = f\"{i}\"\n",
    "        r = requests.get(url, headers)\n",
    "        \n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        divs = soup.find_all(\"div\", class_=\"csa_jobadText\")        \n",
    "        info_job.append(divs)\n",
    "\n",
    "        # job_ = ''.join(str(i) for i in info_job)\n",
    "        # nltk.word_tokenize(clean_text(job_))\n",
    "        # for word in job_:\n",
    "        #     if word not in stop_word:\n",
    "        #         jobindex.append(word)\n",
    "            \n",
    "    return info_job\n",
    "\n",
    "jobindex_psych = clean_jobindex('cand.psych')\n",
    "# jobindex_oecon = clean_jobindex('cand.polit')\n",
    "# jobindex_pol = clean_jobindex('cand.scient.pol')\n",
    "# jobindex_anth = clean_jobindex('cand.scient.anth')\n",
    "# jobindex_soc = clean_jobindex('cand.scient.soc')\n",
    "jobindex_psych\n",
    "\n",
    "psych = []\n",
    "jobindex_psych = ''.join(str(i) for i in jobindex_psych)\n",
    "job_ = clean_text(jobindex_psych)\n",
    "nltk.word_tokenize(job_)\n",
    "for word in job_:\n",
    "    if word not in stop_word:\n",
    "        psych.append(word)\n",
    "\n",
    "psych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Jobindex data using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "stopwords.update(['hr', 'bringe', 'ppr', 'vores', 'ønsker', 'nye', 'hverdag', 'del', 'inden', 'søger', 'kan', 'egedal', 'oktober', 'kompetencer'])\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, max_font_size=35, max_words=100, background_color='white').generate(psych)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stopwords, max_font_size=35, max_words=100, background_color='white').generate(oecon)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'økonom':'cand.oecon', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecon_esco = jobs.loc[jobs['job_title'] == 'cand.oecon']\n",
    "search_words = []\n",
    "for value in oecon_esco['essential_skill']:\n",
    "    search_words.append(value)\n",
    "    \n",
    "search_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecon.find('statistik')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = nltk.word_tokenize(extract_UG(education_url[0][0]))\n",
    "oecon = nltk.word_tokenize(extract_UG(education_url[1][0]))\n",
    "pol = nltk.word_tokenize(extract_UG(education_url[2][0]))\n",
    "anth = nltk.word_tokenize(extract_UG(education_url[3][0]))\n",
    "soc = nltk.word_tokenize(extract_UG(education_url[4][0]))\n",
    "psych_final = []\n",
    "for word in psych:\n",
    "    if word not in stop_word:\n",
    "        psych_final.append(word)\n",
    "\n",
    "psych_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_text('psych', psych_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_text = tf.data.TextLineDataset('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS_edit/Exam/psych.txt').filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(psych_text.batch(4096))\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = psych_text.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in sequences[:10]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
