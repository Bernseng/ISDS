{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for data scrabing on Esco and Jobnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !python -m nltk.downloader popular\n",
    "# !pip install tensorflow\n",
    "# !pip install spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "stop_word = nltk.corpus.stopwords.words('danish')\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "import string\n",
    "from function import *\n",
    "\n",
    "output = Path(r'/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS_edit/Exam/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data on Jobindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobindex(page, tag):\n",
    "    \n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    url = f\"https://www.jobindex.dk/jobsoegning?page={page}&q={tag}\"\n",
    "                    \n",
    "    r = requests.get(url, headers)\n",
    "        \n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "                    \n",
    "    divs = soup.find_all(\"div\", class_=\"jobsearch-result\")         \n",
    "\n",
    "    for item in divs:\n",
    "        title = item.find_all(\"b\")[0].text.strip()\n",
    "        #company = item.find_all(\"b\")[1].text.strip()\n",
    "        #published_date = item.find(\"time\").text.strip()\n",
    "        summary = item.find_all(\"p\")[1].text.strip()\n",
    "        #job_location = item.find_all(\"p\")[0].text.strip()\n",
    "        #job_url =  item.select_one('[data-click*=\"u=\"]:has(> b)')['href']\n",
    "                        \n",
    "\n",
    "        job = {\n",
    "        \"job_title\" : tag,\n",
    "        \"title\" : title, \n",
    "        #\"company\" : company,\n",
    "        #\"published_date\" : published_date,\n",
    "        \"summary\" : summary,\n",
    "        #\"job_location\" : job_location,\n",
    "        #\"job_url\" : job_url\n",
    "        }\n",
    "\n",
    "        job_list.append(job)\n",
    "        \n",
    "    return\n",
    "\n",
    "search_list = ['cand.psych', 'cand.polit', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "job_list = []\n",
    "for k in search_list:  \n",
    "    for i in range(100):\n",
    "        try: \n",
    "            job_df = extract_jobindex(i,k)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "job_df = pd.DataFrame(data=job_list, columns=['job_title', \"title\", \"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'Ã¸konom':'cand.polit', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = nltk.word_tokenize(extract_UG(education_url[0][0]))\n",
    "oecon = nltk.word_tokenize(extract_UG(education_url[1][0]))\n",
    "pol = nltk.word_tokenize(extract_UG(education_url[2][0]))\n",
    "anth = nltk.word_tokenize(extract_UG(education_url[3][0]))\n",
    "soc = nltk.word_tokenize(extract_UG(education_url[4][0]))\n",
    "psych_final = []\n",
    "for word in psych:\n",
    "    if word not in stop_word:\n",
    "        psych_final.append(word)\n",
    "\n",
    "psych_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text('psych', psych_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych_text = tf.data.TextLineDataset('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS_edit/Exam/psych.txt').filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(psych_text.batch(4096))\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = psych_text.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in sequences[:10]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
