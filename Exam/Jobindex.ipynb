{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for data scrabing on Esco and Jobnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "# !python -m nltk.downloader popular\n",
    "# !pip install tensorflow\n",
    "# !pip install spacy\n",
    "# !pip install wordloud\n",
    "# !python -m spacy download da_core_news_md\n",
    "# !pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tensorflow.keras import layers\n",
    "from spacy.lang.da.stop_words import STOP_WORDS\n",
    "\n",
    "import string\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from function import *\n",
    "\n",
    "seed = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopord.txt', encoding='utf8') as f:\n",
    "    stopord = f.read().splitlines()\n",
    "stop_word = nltk.corpus.stopwords.words('danish') + stopord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "['hr','bringe','ppr','vores','ønsker','nye','hverdag','del','inden','søger', \n",
    "'kan','egedal','oktober','kompetencer','li','ul','text','align','justify','br','p','div','strong',\n",
    "'style','href','både','ung','halsnæs','egen','daglig','hel','vigtig','relevant','velkommen','tidlig','første','enkelt','tæt','flest',\n",
    "'høj','ny','fast','mulig','lille','øvrig','mange','meget','central','voksen','stor','gammel','konkret','aktuel','ugenligt','gavn','formår',\n",
    "'god','vant','int','fig','centralt','evalueres','mittag','løbende','spise','nær','nemmere']\n",
    "\n",
    "nlp = spacy.load('da_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data on Jobindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Empty\n",
    "\n",
    "def extract_jobindex(tag, pages):\n",
    "    flat_list = []\n",
    "    url_list = []\n",
    "    total_pages = range(pages)\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "\n",
    "    for page in total_pages:\n",
    "        url = f\"https://www.jobindex.dk/jobsoegning?maxdate=20220101&mindate=20210101&page={page}&jobage=archive&q={tag}\"\n",
    "                        \n",
    "        r = requests.get(url, headers)\n",
    "            \n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "                        \n",
    "        divs = soup.find_all(\"div\", class_=\"jobsearch-result\")\n",
    "        \n",
    "        for item in divs:\n",
    "            try:\n",
    "                job_url = item.select_one('[data-click*=\"u=\"]:has(> b)')['data-click']\n",
    "                url_list.append(job_url)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for i in url_list:\n",
    "            link = 'http://www.jobindex.dk' + i\n",
    "            flat_list.append(link)\n",
    "\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Jobindex data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "# nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jobindex(search_word):\n",
    "    final_job = []\n",
    "    jobindex = []\n",
    "    info_job = [] \n",
    "    flat_list = []\n",
    "    final_list = []\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "    job = extract_jobindex(search_word, 4)\n",
    "    \n",
    "    for i in job:\n",
    "        url = f\"{i}\"\n",
    "        r = requests.get(url, headers)\n",
    "        soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "        iframe = soup.find_all('iframe', class_='archive-content')\n",
    "        for i in iframe:\n",
    "            link = i['src']\n",
    "            info_job.append(link)\n",
    "\n",
    "    for item in info_job:\n",
    "        links = 'http://www.jobindexarkiv.dk/cgi/showarchive.cgi' + item\n",
    "        jobindex.append(links)\n",
    "\n",
    "\n",
    "    for i in jobindex:\n",
    "        url_ = f\"{i}\"\n",
    "        r = requests.get(url_, headers)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        content = soup.find_all('body')\n",
    "        for i in content:\n",
    "            text = i.get_text()\n",
    "            flat_list.append(text)\n",
    "\n",
    "    job_ = ' '.join(str(i) for i in flat_list)\n",
    "    _job = nltk.word_tokenize(clean_text(job_))\n",
    "    for word in _job:\n",
    "        if word not in stop_word:\n",
    "            final_job.append(word)\n",
    "            \n",
    "    return final_job\n",
    "\n",
    "jobindex_psych = [wnl.lemmatize(i) for i in clean_jobindex('cand.psych')]\n",
    "jobindex_oecon = [wnl.lemmatize(i) for i in clean_jobindex('cand.oecon')]\n",
    "jobindex_pol = [wnl.lemmatize(i) for i in clean_jobindex('cand.scient.pol')]\n",
    "jobindex_anth = [wnl.lemmatize(i) for i in clean_jobindex('cand.scient.anth')]\n",
    "jobindex_soc = [wnl.lemmatize(i) for i in clean_jobindex('cand.scient.soc')]\n",
    "\n",
    "jobindex_psych_full = ' '.join(jobindex_psych)\n",
    "jobindex_oecon_full = ' '.join(jobindex_oecon)\n",
    "jobindex_pol_full = ' '.join(jobindex_pol)\n",
    "jobindex_anth_full = ' '.join(jobindex_anth)\n",
    "jobindex_soc_full = ' '.join(jobindex_soc)\n",
    "\n",
    "print(f'Number of word in \\n psych: {len(jobindex_psych_full)}, \\n oecon: {len(jobindex_oecon_full)}, \\n pol: {len(jobindex_pol_full)}, \\n anth: {len(jobindex_anth_full)}, \\n soc: {len(jobindex_soc_full)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Jobindex data using Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_text('cand.oecon', jobindex_oecon)\n",
    "write_text('cand.psych', jobindex_psych)\n",
    "write_text('cand.pol', jobindex_pol)\n",
    "write_text('cand.anth', jobindex_anth)\n",
    "write_text('cand.soc', jobindex_soc)\n",
    "\n",
    "def stacy_words(input):\n",
    "    document = nlp(open(f'{input}', encoding=\"utf-8\").read())\n",
    "    adjs = []\n",
    "    for token in document:\n",
    "       if token.pos_ == 'ADJ':\n",
    "        adjs.append(token.lemma_)\n",
    "    # adjs_tally = Counter(adjs)\n",
    "    # adjs_tally.most_common()\n",
    "        adj = ' '.join(adjs)\n",
    "    return adj\n",
    "\n",
    "path = Path('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS/Exam/')\n",
    "\n",
    "oecon_adjs = stacy_words(path / 'cand.oecon.txt')\n",
    "psych_adjs = stacy_words(path / 'cand.psych.txt')\n",
    "pol_adjs = stacy_words(path / 'cand.pol.txt')\n",
    "anth_adjs = stacy_words(path / 'cand.anth.txt')\n",
    "soc_adjs = stacy_words(path / 'cand.soc.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(oecon_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(psych_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(pol_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(anth_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stop_word, max_font_size=35, max_words=100, background_color='white').generate(soc_adjs)\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'økonom':'cand.oecon', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_search_words(df, type_):\n",
    "    search_words = []\n",
    "    searchword = df.loc[jobs['job_title'] == f'{type_}']\n",
    "    for value in searchword['optional_skill']:\n",
    "        search_words.append(value)\n",
    "    for value in searchword['essential_skill']:\n",
    "        search_words.append(value)\n",
    "    return search_words\n",
    "\n",
    "esco_oecon = find_search_words(jobs, 'cand.oecon')\n",
    "esco_psych = find_search_words(jobs, 'cand.psych')\n",
    "esco_pol = find_search_words(jobs, 'cand.scient.pol')\n",
    "esco_anth = find_search_words(jobs, 'cand.scient.anth')\n",
    "esco_soc = find_search_words(jobs, 'cand.scient.soc')\n",
    "\n",
    "print(f'Number of skills in: \\n psych: {len(esco_psych)}, \\n oecon: {len(esco_oecon)}, \\\n",
    "    \\n pol: {len(esco_pol)}, \\n anth: {len(esco_anth)}, \\n soc: {len(esco_soc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psych = nltk.word_tokenize(extract_UG(education_url[0][0]))\n",
    "# oecon = nltk.word_tokenize(extract_UG(education_url[1][0]))\n",
    "# pol = nltk.word_tokenize(extract_UG(education_url[2][0]))\n",
    "# anth = nltk.word_tokenize(extract_UG(education_url[3][0]))\n",
    "# soc = nltk.word_tokenize(extract_UG(education_url[4][0]))\n",
    "# psych_final = []\n",
    "# for word in psych:\n",
    "#     if word not in stop_word:\n",
    "#         psych_final.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ku_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'statskundskab', 'antropologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    soup = extract_ku(k)\n",
    "    ku_list.append(transform_ku(soup))\n",
    "\n",
    "ku_df = pd.DataFrame(data=ku_list)\n",
    "\n",
    "#Merging the two colums \n",
    "ku_df=ku_df[0]+ku_df[1]\n",
    "\n",
    "#Making the object a dataframe\n",
    "ku_df=pd.DataFrame(ku_df)\n",
    "\n",
    "ku_df=pd.DataFrame.transpose(ku_df)\n",
    "\n",
    "ku_df.columns=['cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', 'cand.oecon_ku']\n",
    "soup = extract_au('statskundskab')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_stats = text[75:77]\n",
    "\n",
    "stats_df = pd.DataFrame(data=text_stats, columns=['cand.scient.pol_au'])\n",
    "stats = pd.DataFrame([', '.join(stats_df['cand.scient.pol_au'].to_list())], columns=['cand.scient.pol_au'])\n",
    "soup = extract_au('oekonomi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_oek = text[60:62]\n",
    "\n",
    "oek_df = pd.DataFrame(data=text_oek, columns=['cand.oecon_au'])\n",
    "oek = pd.DataFrame([', '.join(oek_df['cand.oecon_au'].to_list())], columns=['cand.oecon_au'])\n",
    "soup = extract_au('antropologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_ant = text[50:55]\n",
    "\n",
    "ant_df = pd.DataFrame(data=text_ant, columns=['cand.scient.anth_au'])\n",
    "ant = pd.DataFrame([', '.join(ant_df['cand.scient.anth_au'].to_list())], columns=['cand.scient.anth_au'])\n",
    "soup = extract_au('psykologi')\n",
    "\n",
    "divs = soup.find_all(\"div\", class_=\"large-8 medium-8 medium-only-portrait-12 small-12 columns\")\n",
    "\n",
    "text = soup.find_all('p')\n",
    "\n",
    "text_psyk = text[74:78]\n",
    "\n",
    "psyk_df = pd.DataFrame(data=text_psyk, columns=['cand.psych_au'])\n",
    "psyk = pd.DataFrame([', '.join(psyk_df['cand.psych_au'].to_list())], columns=['cand.psych_au'])\n",
    "frames = [ant, stats, psyk, oek]\n",
    "\n",
    "au_df = pd.concat(frames, axis=1)\n",
    "\n",
    "au_df_list = au_df['cand.scient.anth_au'].to_list()\n",
    "au_list = []\n",
    "\n",
    "for i in au_df_list:\n",
    "    au_list.append(clean_text(i))\n",
    "\n",
    "au_list\n",
    "def transform_aau(soup):\n",
    "\n",
    "    divs = soup.find_all(\"main\", class_=\"Main_Main__2KIvG\")\n",
    "\n",
    "    for item in divs:\n",
    "        text_aau = item.find_all()[0].text.strip()\n",
    "\n",
    "        aau_text = {\n",
    "            \"text_aau\" : text_aau, \n",
    "        }\n",
    "        aau_list.append(aau_text)\n",
    "\n",
    "        text_aau = clean_text(text_aau)\n",
    "        \n",
    "    return text_aau\n",
    "\n",
    "aau_list = []\n",
    "\n",
    "search_list = ['psykologi', 'sociologi', 'oekonomi']\n",
    "\n",
    "for k in search_list:\n",
    "    try: \n",
    "        soup = extract_aau(k)\n",
    "        transform_aau(soup)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "aau_df = pd.DataFrame(data=aau_list)\n",
    "\n",
    "aau_df=pd.DataFrame.transpose(aau_df)\n",
    "\n",
    "aau_df.columns=['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau']\n",
    "\n",
    "aau_df = aau_df.reset_index(drop=True)\n",
    "\n",
    "aau_df\n",
    "\n",
    "## Combining Dataframes\n",
    "merge_frames = [ku_df, au_df, aau_df]\n",
    "\n",
    "combined_df = pd.concat(merge_frames, axis=1)\n",
    "\n",
    "combined_df['cand.psych_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.psych_ku']]\n",
    "combined_df['cand.scient.pol_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.pol_ku']]\n",
    "combined_df['cand.oecon_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.oecon_ku']]\n",
    "combined_df['cand.scient.anth_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.anth_ku']]\n",
    "combined_df['cand.scient.soc_ku_string'] = [','.join(map(str, l)) for l in combined_df['cand.scient.soc_ku']]\n",
    "\n",
    "combined_df['cand.psych'] = combined_df['cand.psych_aau'] + combined_df['cand.psych_au'] + combined_df['cand.psych_ku_string']\n",
    "combined_df['cand.scient.anth'] = combined_df['cand.scient.anth_au'] + combined_df['cand.scient.anth_ku_string']\n",
    "combined_df['cand.scient.pol'] = combined_df['cand.scient.pol_au'] + combined_df['cand.scient.pol_ku_string']\n",
    "combined_df['cand.scient.soc'] = combined_df['cand.scient.soc_aau'] + combined_df['cand.scient.soc_ku_string']\n",
    "combined_df['cand.oecon'] = combined_df['cand.oecon_aau'] + combined_df['cand.oecon_au'] + combined_df['cand.oecon_ku_string']\n",
    "combined_df.drop(['cand.psych_aau', 'cand.scient.soc_aau', 'cand.oecon_aau', 'cand.psych_au', \\\n",
    "                  'cand.scient.anth_au', 'cand.scient.pol_au','cand.oecon_au', \\\n",
    "                  'cand.psych_ku', 'cand.scient.soc_ku', 'cand.scient.pol_ku', 'cand.scient.anth_ku', \\\n",
    "                  'cand.oecon_ku', 'cand.psych_ku_string', 'cand.scient.pol_ku_string', 'cand.oecon_ku_string', \\\n",
    "                  'cand.scient.anth_ku_string', 'cand.scient.soc_ku_string'], axis=1, inplace=True)\n",
    "university_df = combined_df\n",
    "university_df\n",
    "\n",
    "## Scraping UG\n",
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]\n",
    "psych = clean_text(extract_UG(education_url[0][0]))\n",
    "oecon = extract_UG(education_url[1][0])\n",
    "pol = extract_UG(education_url[2][0])\n",
    "anth = extract_UG(education_url[3][0])\n",
    "soc = extract_UG(education_url[4][0])\n",
    "\n",
    "## Combining skills from UG and Universities\n",
    "strings_psych = psych + university_df['cand.psych']\n",
    "strings_oecon = oecon + university_df['cand.oecon']\n",
    "strings_pol = pol + university_df['cand.scient.pol']\n",
    "strings_anth = anth + university_df['cand.scient.anth']\n",
    "strings_soc = soc + university_df['cand.scient.soc']\n",
    "\n",
    "\n",
    "psych_comb = \" \".join(strings_psych)\n",
    "oecon_comb = \" \".join(strings_oecon)\n",
    "pol_comb = \" \".join(strings_pol)\n",
    "anth_comb = \" \".join(strings_anth)\n",
    "soc_comb = \" \".join(strings_soc)\n",
    "psych_series = pd.Series(psych_comb)\n",
    "oecon_series = pd.Series(oecon_comb)\n",
    "pol_series = pd.Series(pol_comb)\n",
    "anth_series = pd.Series(anth_comb)\n",
    "soc_series = pd.Series(soc_comb)\n",
    "\n",
    "psych_df = pd.DataFrame(psych_series)\n",
    "oecon_df = pd.DataFrame(oecon_series)\n",
    "pol_df = pd.DataFrame(pol_series)\n",
    "anth_df = pd.DataFrame(anth_series)\n",
    "soc_df = pd.DataFrame(soc_series)\n",
    "\n",
    "psych_df['cand.psych'] = psych_df[0]\n",
    "oecon_df['cand.oecon'] = oecon_df[0]\n",
    "pol_df['cand.scient.pol'] = pol_df[0]\n",
    "anth_df['cand.scient.anth'] = anth_df[0]\n",
    "soc_df['cand.scient.soc'] = soc_df[0]\n",
    "final_df = pd.concat([psych_df['cand.psych'], oecon_df['cand.oecon'], pol_df['cand.scient.pol'], \\\n",
    "                      anth_df['cand.scient.anth'], soc_df['cand.scient.soc']], axis=1)\n",
    "\n",
    "final_df = make_a_list(final_df['cand.psych']).append(make_a_list(final_df['cand.oecon'])).append(make_a_list(final_df['cand.scient.pol']))\\\n",
    "    .append(make_a_list(final_df['cand.scient.anth'])).append(make_a_list(final_df['cand.scient.soc']))\n",
    "\n",
    "final_df = final_df.T\n",
    "final_df.columns = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(nltk.word_tokenize)\n",
    "final_df['cand.oecon'] = final_df['cand.oecon'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(nltk.word_tokenize)\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(nltk.word_tokenize)\n",
    "\n",
    "final_df['cand.psych'] = final_df['cand.psych'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.oecon'] = final_df['cand.oecon'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.pol'] = final_df['cand.scient.pol'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.anth'] = final_df['cand.scient.anth'].apply(lambda words: [word for word in words if word not in stop_word])\n",
    "final_df['cand.scient.soc'] = final_df['cand.scient.soc'].apply(lambda words: [word for word in words if word not in stop_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(series):\n",
    "    final_list_ = []\n",
    "    for i in series:\n",
    "        list_ = list(i)\n",
    "    list_ = ' '.join(list_)\n",
    "    final_list = list_.split(' ')\n",
    "    for word in final_list:\n",
    "        if word not in stop_word:\n",
    "            final_list_.append(word)\n",
    "    return final_list_\n",
    "\n",
    "udd_oecon_list = df_to_list(final_df['cand.oecon'])\n",
    "udd_psych_list = df_to_list(final_df['cand.psych'])\n",
    "udd_pol_list = df_to_list(final_df['cand.scient.pol'])\n",
    "udd_anth_list = df_to_list(final_df['cand.scient.anth'])\n",
    "udd_soc_list = df_to_list(final_df['cand.scient.soc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = oecon_adjs\n",
    "\n",
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels\n",
    "\n",
    "path_to_file = Path('/Users/nicolaibernsen/Desktop/KU/9.Semester/Introduction_to_Social_Datascience/ISDS/Exam/cand.oecon.txt')\n",
    "\n",
    "with open(path_to_file) as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "# def custom_standardization(input_data):\n",
    "#   lowercase = tf.strings.lower(input_data)\n",
    "#   return tf.strings.regex_replace(lowercase,\n",
    "#                                   '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    # standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])\n",
    "\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "\n",
    "print(len(sequences))\n",
    "\n",
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import tqdm'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 373.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (3,)\n",
      "contexts.shape: (3, 5)\n",
      "labels.shape: (3, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=seed)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esco_1_oecon = []\n",
    "for item in esco_oecon:\n",
    "    if len(item.split(' ')) == 1:\n",
    "        esco_1_oecon.append(item)\n",
    "esco_1_oecon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words(words_to_search, list_, name):\n",
    "    for i in words_to_search:\n",
    "        if i in list_:\n",
    "            print(f'{i} is present in the list of {name}')\n",
    "\n",
    "find_words(esco_oecon, jobindex_oecon, 'oecon')\n",
    "find_words(esco_anth, jobindex_anth, 'anth')\n",
    "find_words(esco_pol, jobindex_pol, 'pol')\n",
    "find_words(esco_psych, jobindex_psych, 'psych')\n",
    "find_words(esco_soc, jobindex_soc, 'soc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectoizer(input_udd, input_jobindex):\n",
    "    vector = TfidfVectorizer(lowercase=False)\n",
    "    ret = vector.fit_transform([input_jobindex, input_udd])\n",
    "    return \n",
    "\n",
    "countvector = CountVectorizer(lowercase=False)\n",
    "tfidfvetor = TfidfVectorizer(lowercase=False)\n",
    "\n",
    "esco_oecon = ' '.join(esco_oecon)\n",
    "esco_oecon = esco_oecon.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = countvector.fit_transform(esco_oecon)\n",
    "\n",
    "terms_vector = countvector.transform(jobindex_oecon)\n",
    "\n",
    "feature_names = countvector.get_feature_names()\n",
    "\n",
    "df_1 = pd.DataFrame(terms_vector.todense())\n",
    "\n",
    "# feature_names\n",
    "\n",
    "# df_1.loc[lambda i: i==1]\n",
    "# df_1.to_csv('out.csv')\n",
    "\n",
    "# tfidfvectorizer_oecon.fit_transform(jobindex_oecon, )\n",
    "\n",
    "# feature_names = tfidfvectorizer_oecon.get_feature_names()\n",
    "\n",
    "# dense = vector.todense()\n",
    "\n",
    "# denselist = dense.tolist()\n",
    "\n",
    "# df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "# summarize\n",
    "# print(tfidfvectorizer.idf_)\n",
    "# print(vector_jobindex.idf_)\n",
    "\n",
    "# lr_regression = LogisticRegression(random_state=0) #Text classifier\n",
    "\n",
    "# lr_regression.fit(vector_jobindex, jobindex_oecon)\n",
    "\n",
    "# preds = lr_regression.predict(vector_jobindex)\n",
    "\n",
    "# lr_regression.fit(vector_jobindex, vector_udd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing words with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "def bert_model_simmilarities(udd_list, jobindex_list):\n",
    "    list_ = []\n",
    "    document_embeddings = sbert_model.encode([udd_list, jobindex_list])\n",
    "    similarities = cosine_similarity(document_embeddings)\n",
    "    for cell in np.nditer(similarities):\n",
    "        if cell < 0.9 and cell > 0.1:\n",
    "            list_.append(cell)\n",
    "\n",
    "    return list_[1].tolist()\n",
    "\n",
    "def bert_model_differences(udd_list, jobindex_list):\n",
    "    list_ = []\n",
    "    document_embeddings = sbert_model.encode([udd_list, jobindex_list])\n",
    "    differences = euclidean_distances(document_embeddings)\n",
    "\n",
    "    for cell in np.nditer(differences):\n",
    "        if cell > 0.1:\n",
    "            list_.append(cell)\n",
    "\n",
    "    return list_[1].tolist()\n",
    "\n",
    "arr_oecon = bert_model_differences(udd_oecon_list, jobindex_oecon)\n",
    "arr_oecon_2 = bert_model_simmilarities(udd_oecon_list, jobindex_oecon)\n",
    "\n",
    "arr_psych = bert_model_differences(udd_psych_list, jobindex_psych)\n",
    "arr_psych_2 = bert_model_simmilarities(udd_psych_list, jobindex_psych)\n",
    "\n",
    "arr_pol = bert_model_differences(udd_pol_list, jobindex_pol)\n",
    "arr_pol_2 = bert_model_simmilarities(udd_pol_list, jobindex_pol)\n",
    "\n",
    "arr_soc = bert_model_differences(udd_soc_list, jobindex_soc)\n",
    "arr_soc_2 = bert_model_simmilarities(udd_soc_list, jobindex_soc)\n",
    "\n",
    "arr_anth = bert_model_differences(udd_anth_list, jobindex_anth)\n",
    "arr_anth_2 = bert_model_simmilarities(udd_anth_list, jobindex_anth)\n",
    "\n",
    "list_ = (arr_oecon, arr_psych, arr_pol, arr_soc, arr_anth)\n",
    "_list = (arr_oecon_2, arr_psych_2, arr_pol_2, arr_soc_2, arr_anth_2)\n",
    "\n",
    "df_diff = pd.DataFrame(list_)\n",
    "\n",
    "df_sim = pd.DataFrame(_list)\n",
    "\n",
    "df = pd.concat([df_diff, df_sim], axis=1)\n",
    "df = df.rename(index={0:'cand.oecon', 1:'cand.psych', 2:'cand.scient.pol', 3:'cand.scient.soc', 4:'cand.scient.anth'})\n",
    "df.columns=['Euclidean Distance', 'Cosine Similarity']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
