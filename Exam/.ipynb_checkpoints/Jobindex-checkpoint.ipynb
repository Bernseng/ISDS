{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for data scrabing on Esco and Jobnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import magics\n",
    "\n",
    "# !pip install selenium\n",
    "# !pip install webdriver_manager\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "\n",
    "output = Path(r'C:\\Users\\TheNixe\\Desktop\\KU\\ISDS\\ISDS_edit\\Exam\\output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting data on Jobindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_jobindex(page, tag):\n",
    "    \n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    url = f\"https://www.jobindex.dk/jobsoegning?page={page}&q={tag}\"\n",
    "                    \n",
    "    r = requests.get(url, headers)\n",
    "        \n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "                    \n",
    "    divs = soup.find_all(\"div\", class_=\"jobsearch-result\")\n",
    "                    \n",
    "\n",
    "    for item in divs:\n",
    "        title = item.find_all(\"b\")[0].text.strip()\n",
    "        #company = item.find_all(\"b\")[1].text.strip()\n",
    "        #published_date = item.find(\"time\").text.strip()\n",
    "        summary = item.find_all(\"p\")[1].text.strip()\n",
    "        #job_location = item.find_all(\"p\")[0].text.strip()\n",
    "        #job_url =  item.select_one('[data-click*=\"u=\"]:has(> b)')['href']\n",
    "                        \n",
    "\n",
    "        job = {\n",
    "        \"job_title\" : tag,\n",
    "        \"title\" : title, \n",
    "        #\"company\" : company,\n",
    "        #\"published_date\" : published_date,\n",
    "        \"summary\" : summary,\n",
    "        #\"job_location\" : job_location,\n",
    "        #\"job_url\" : job_url\n",
    "        }\n",
    "\n",
    "        job_list.append(job)\n",
    "        \n",
    "    return\n",
    "\n",
    "job_list = []\n",
    "search_list = ['cand.psych', 'cand.polit', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "for k in search_list:  \n",
    "    for i in range(100):\n",
    "        try: \n",
    "            job_df = extract_jobindex(i,k)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "job_df = pd.DataFrame(data=job_list, columns=['job_title', \"title\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing skills from ESCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetching_occupation(uri):\n",
    "\n",
    "    url = f'http://ec.europa.eu/esco/api/resource/occupation?uri={uri}&language=da'\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    result = response.json()\n",
    "\n",
    "    job_title = result['title']\n",
    "    joblist = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        try:\n",
    "            essential_skill = result['_links']['hasEssentialSkill'][i]['title']\n",
    "            optional_skill = result['_links']['hasOptionalSkill'][i]['title']\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        job = {\n",
    "        'job_title' : job_title,\n",
    "        'essential_skill': essential_skill,\n",
    "        'optional_skill' : optional_skill\n",
    "        }   \n",
    "        joblist.append(job)\n",
    "\n",
    "        jobs = pd.DataFrame(data=joblist, columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "        # jobs = jobs.set_index(['title'], append=True).swaplevel(0,1)\n",
    "\n",
    "    return jobs\n",
    "    \n",
    "\n",
    "occupations = ['http://data.europa.eu/esco/occupation/99492920-e5a5-4dba-9e5a-93193147198c', \n",
    "'http://data.europa.eu/esco/occupation/11df8941-508c-4103-ad40-52cdf9430a59', \n",
    "'http://data.europa.eu/esco/occupation/acf69cab-8629-45c8-ae10-c8fb15f474b6', \n",
    "'http://data.europa.eu/esco/occupation/52ded7d7-11df-42e3-b90a-d7f4b70fb4b9',\n",
    "'http://data.europa.eu/esco/occupation/4f89b0d2-b666-4890-af01-25d1d60da1f3']\n",
    "\n",
    "jobs = pd.DataFrame(columns=['job_title', 'essential_skill', 'optional_skill'])\n",
    "\n",
    "for i in occupations:\n",
    "    jobs = jobs.append(fetching_occupation(i))\n",
    "\n",
    "jobs = jobs.apply(lambda x: x.replace({'Ã¸konom':'cand.polit', 'psykolog':'cand.psych', 'antropolog':'cand.scient.anth', \n",
    "'politolog':'cand.scient.pol', 'sociolog':'cand.scient.soc'}, regex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping KU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ku_list = ['https://studier.ku.dk/kandidat/oekonomi/faglig-profil-og-job/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping UG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UG(page, tag):\n",
    "    \n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "    \n",
    "    url = f\"https://www.ug.dk/search/{tag}?page={page}\"\n",
    "    \n",
    "    r = requests.get(url, headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    divs = soup.find_all(\"div\", class_=\"node node-uddannelse node-teaser clearfix\")\n",
    "\n",
    "    list_of_articles = []\n",
    "\n",
    "    for i in range(len(divs)):\n",
    "        list_of_articles.append(divs[i].find('a')['href'])\n",
    "    \n",
    "    list_of_articles_final = []\n",
    "    for link in list_of_articles:\n",
    "        if '/kandidatuddannelser/' in link:\n",
    "            list_of_articles_final.append(link)\n",
    "\n",
    "    # list_of_articles = ['ug.dk' + s for s in list_of_articles]\n",
    "\n",
    "    return list_of_articles_final\n",
    "\n",
    "\n",
    "search_list = ['cand.psych', 'cand.oecon', 'cand.scient.pol', 'cand.scient.anth', 'cand.scient.soc']\n",
    "\n",
    "education_url = []\n",
    "\n",
    "for k in search_list:\n",
    "    for i in range(1):\n",
    "        try: \n",
    "            education_url.append(UG(i,k))\n",
    "        except:\n",
    "            break\n",
    "        \n",
    "education_url[1].pop(0)\n",
    "del education_url[4][0:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/uddannelser/bachelorogkandidatuddannelser/kandidatuddannelser/samfundsvidenskabeligekandidatuddannelser/erhvervsoekonomi/oekonomi'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "education_url[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_UG(link):\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    url = 'http://ug.dk' + link\n",
    "\n",
    "    r = requests.get(url, headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "    \n",
    "    divs_intro = soup.find_all(\"div\", class_=\"pane-content\")\n",
    "\n",
    "    temp_intro = soup.find_all('p')\n",
    "\n",
    "    divs_main = soup.find_all(\"div\", class_=\"field-content\")\n",
    "\n",
    "    temp_main = soup.find_all('li')\n",
    "    \n",
    "    main_text = []\n",
    "    main_text.append(temp_main)\n",
    "    main_text.append(temp_intro)\n",
    "    main_text = ' '.join(str(i) for i in main_text)\n",
    "\n",
    "\n",
    "    return main_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('[','').replace(']', '')\n",
    "    text = re.sub(\"(<li>)|(</li>)|</p>|<p>|\\xa0|<ul>|</ul>|'|-| +|,| ,\", ' ', text)\n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    return text\n",
    "    \n",
    "def extract_UG(link):\n",
    "\n",
    "    headers = {'User-Agent':'kjp538@alumni.ku.dk'}\n",
    "\n",
    "    url = 'http://ug.dk' + link\n",
    "\n",
    "    r = requests.get(url, headers)\n",
    "\n",
    "    soup = BeautifulSoup(r.content.decode(\"utf-8\"), \"html.parser\")\n",
    "\n",
    "    divs_intro = soup.find_all(\"div\", class_=\"region region-content\")\n",
    "    for item in divs_intro:\n",
    "        intro = item.find_all(\"p\")[0:6]\n",
    "        indhold = item.find_all('ul')[0:1]\n",
    "        outro = item.find_all('p')[19:22]\n",
    "    \n",
    "    text = ' '.join([str(i) for i in intro]) + ' '.join([str(i) for i in indhold]) + ' '.join([str(i) for i in outro])\n",
    "    text = clean_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = extract_UG(education_url[0][0])\n",
    "oecon = extract_UG(education_url[1][0])\n",
    "pol = extract_UG(education_url[2][0])\n",
    "anth = extract_UG(education_url[3][0])\n",
    "soc = extract_UG(education_url[4][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f391330d4b460aafb87b0964baa3bc9feb41f83abb3c41e72e23789a7e646ea5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
